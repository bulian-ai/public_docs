{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison of Synthetic Data for Prediction Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are working with data for cardiovascular disease for 70000 different patient. Based on the features present in the dataset we make binary classification of whether the patient has cardiovascular disease or not. The intial scoring is done on pure real data followed by scoring on pure synthetic data and a combination of real and synthetic data in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from bulian.Tabular.synthesizers import TwinSynthesizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple helper function `get_f1_score` which helps us to get the f1_score for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(X, Y, model):\n",
    "    preds = model.predict(X)\n",
    "    return f1_score(Y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/anurag/Tabular_Synthesizers/examples/csv/cardio_train.csv\", sep=\";\")\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "Y_full = df['cardio'].dropna()\n",
    "X_full = df.drop(['cardio'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataset was retrieved from - https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(learning_rate=0.1, colsample_bytree=0.8, n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
       "              n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X_full, Y_full, train_size=0.7, test_size=0.3, random_state=0)\n",
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Real Test Data -  0.7192912602030659\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score on Real Test Data - \", get_f1_score(test_X, test_Y, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of TwinSynthesizer model to generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/349]  eta: 0:00:19  loss_g: 0.0377 (0.0377)  loss_d: -0.0125 (-0.0125)  loss: 0.0252 (0.0252)  time: 0.0558  data: 0.0000\n",
      "Epoch: [0]  [ 50/349]  eta: 0:00:12  loss_g: -0.5799 (-0.2457)  loss_d: 0.0715 (-0.0088)  loss: -0.4804 (-0.2545)  time: 0.0414  data: 0.0000\n",
      "Epoch: [0]  [100/349]  eta: 0:00:09  loss_g: -0.2984 (-0.3274)  loss_d: -0.0541 (-0.0138)  loss: -0.3210 (-0.3412)  time: 0.0337  data: 0.0000\n",
      "Epoch: [0]  [150/349]  eta: 0:00:07  loss_g: -0.4459 (-0.3480)  loss_d: 0.0835 (0.0227)  loss: -0.3644 (-0.3253)  time: 0.0339  data: 0.0000\n",
      "Epoch: [0]  [200/349]  eta: 0:00:05  loss_g: -0.4767 (-0.3791)  loss_d: -0.2333 (0.0085)  loss: -0.6772 (-0.3705)  time: 0.0485  data: 0.0000\n",
      "Epoch: [0]  [250/349]  eta: 0:00:03  loss_g: -0.9680 (-0.4695)  loss_d: 0.0854 (0.0203)  loss: -0.8657 (-0.4492)  time: 0.0298  data: 0.0000\n",
      "Epoch: [0]  [300/349]  eta: 0:00:01  loss_g: -0.8542 (-0.5576)  loss_d: -0.1127 (0.0155)  loss: -0.9707 (-0.5421)  time: 0.0371  data: 0.0000\n",
      "Epoch: [0]  [349/349]  eta: 0:00:00  loss_g: -1.2604 (-0.6334)  loss_d: 0.0287 (0.0137)  loss: -1.2163 (-0.6197)  time: 0.0308  data: 0.0000\n",
      "Epoch: [0] Total time: 0:00:12\n",
      "Epoch: [1]  [  0/349]  eta: 0:00:10  loss_g: -1.5193 (-1.5193)  loss_d: 0.2107 (0.2107)  loss: -1.3086 (-1.3086)  time: 0.0288  data: 0.0000\n",
      "Epoch: [1]  [ 50/349]  eta: 0:00:08  loss_g: -1.2811 (-1.3637)  loss_d: -0.1950 (-0.0404)  loss: -1.4346 (-1.4041)  time: 0.0237  data: 0.0000\n",
      "Epoch: [1]  [100/349]  eta: 0:00:07  loss_g: -1.3406 (-1.3638)  loss_d: 0.0167 (0.0005)  loss: -1.3133 (-1.3633)  time: 0.0361  data: 0.0000\n",
      "Epoch: [1]  [150/349]  eta: 0:00:06  loss_g: -1.6470 (-1.4334)  loss_d: -0.0404 (-0.0028)  loss: -1.6632 (-1.4361)  time: 0.0359  data: 0.0000\n",
      "Epoch: [1]  [200/349]  eta: 0:00:04  loss_g: -1.6030 (-1.4891)  loss_d: 0.0011 (-0.0036)  loss: -1.6019 (-1.4927)  time: 0.0380  data: 0.0000\n",
      "Epoch: [1]  [250/349]  eta: 0:00:03  loss_g: -1.6848 (-1.5305)  loss_d: -0.0453 (-0.0060)  loss: -1.7201 (-1.5365)  time: 0.0392  data: 0.0000\n",
      "Epoch: [1]  [300/349]  eta: 0:00:01  loss_g: -1.9103 (-1.5981)  loss_d: -0.0197 (-0.0058)  loss: -1.9247 (-1.6039)  time: 0.0429  data: 0.0000\n",
      "Epoch: [1]  [349/349]  eta: 0:00:00  loss_g: -1.8883 (-1.6283)  loss_d: -0.0139 (-0.0072)  loss: -1.8932 (-1.6355)  time: 0.0437  data: 0.0000\n",
      "Epoch: [1] Total time: 0:00:12\n",
      "Epoch: [2]  [  0/349]  eta: 0:00:12  loss_g: -2.0591 (-2.0591)  loss_d: -0.0427 (-0.0427)  loss: -2.1018 (-2.1018)  time: 0.0361  data: 0.0000\n",
      "Epoch: [2]  [ 50/349]  eta: 0:00:14  loss_g: -2.1550 (-2.0835)  loss_d: -0.0651 (-0.0454)  loss: -2.1987 (-2.1289)  time: 0.0550  data: 0.0000\n",
      "Epoch: [2]  [100/349]  eta: 0:00:11  loss_g: -1.9144 (-2.0213)  loss_d: 0.0370 (-0.0131)  loss: -1.9207 (-2.0343)  time: 0.0428  data: 0.0000\n",
      "Epoch: [2]  [150/349]  eta: 0:00:09  loss_g: -2.2423 (-2.0887)  loss_d: 0.0751 (-0.0049)  loss: -2.2458 (-2.0936)  time: 0.0421  data: 0.0000\n",
      "Epoch: [2]  [200/349]  eta: 0:00:06  loss_g: -2.0898 (-2.1186)  loss_d: 0.0126 (-0.0010)  loss: -2.1010 (-2.1196)  time: 0.0482  data: 0.0000\n",
      "Epoch: [2]  [250/349]  eta: 0:00:04  loss_g: -2.3223 (-2.1548)  loss_d: -0.0998 (-0.0133)  loss: -2.4060 (-2.1682)  time: 0.0406  data: 0.0000\n",
      "Epoch: [2]  [300/349]  eta: 0:00:02  loss_g: -2.2228 (-2.1592)  loss_d: -0.0577 (-0.0117)  loss: -2.2647 (-2.1709)  time: 0.0360  data: 0.0000\n",
      "Epoch: [2]  [349/349]  eta: 0:00:00  loss_g: -2.3348 (-2.1774)  loss_d: 0.0069 (-0.0095)  loss: -2.2965 (-2.1869)  time: 0.0282  data: 0.0000\n",
      "Epoch: [2] Total time: 0:00:14\n",
      "Epoch: [3]  [  0/349]  eta: 0:00:09  loss_g: -2.3285 (-2.3285)  loss_d: -0.0771 (-0.0771)  loss: -2.4056 (-2.4056)  time: 0.0278  data: 0.0000\n",
      "Epoch: [3]  [ 50/349]  eta: 0:00:09  loss_g: -2.3518 (-2.4174)  loss_d: 0.0202 (-0.0078)  loss: -2.3744 (-2.4252)  time: 0.0369  data: 0.0000\n",
      "Epoch: [3]  [100/349]  eta: 0:00:08  loss_g: -2.3052 (-2.3555)  loss_d: -0.0182 (0.0021)  loss: -2.2780 (-2.3535)  time: 0.0285  data: 0.0000\n",
      "Epoch: [3]  [150/349]  eta: 0:00:06  loss_g: -2.1542 (-2.2980)  loss_d: -0.0163 (-0.0047)  loss: -2.1920 (-2.3026)  time: 0.0266  data: 0.0000\n",
      "Epoch: [3]  [200/349]  eta: 0:00:04  loss_g: -2.2920 (-2.2823)  loss_d: -0.0479 (-0.0097)  loss: -2.3350 (-2.2920)  time: 0.0276  data: 0.0000\n",
      "Epoch: [3]  [250/349]  eta: 0:00:02  loss_g: -2.1695 (-2.2718)  loss_d: 0.0397 (-0.0048)  loss: -2.1609 (-2.2766)  time: 0.0271  data: 0.0000\n",
      "Epoch: [3]  [300/349]  eta: 0:00:01  loss_g: -2.1073 (-2.2396)  loss_d: -0.0615 (-0.0065)  loss: -2.1775 (-2.2461)  time: 0.0303  data: 0.0000\n",
      "Epoch: [3]  [349/349]  eta: 0:00:00  loss_g: -2.3129 (-2.2508)  loss_d: 0.0198 (-0.0084)  loss: -2.2616 (-2.2592)  time: 0.0259  data: 0.0000\n",
      "Epoch: [3] Total time: 0:00:10\n",
      "Epoch: [4]  [  0/349]  eta: 0:00:09  loss_g: -2.4427 (-2.4427)  loss_d: -0.0137 (-0.0137)  loss: -2.4564 (-2.4564)  time: 0.0270  data: 0.0000\n",
      "Epoch: [4]  [ 50/349]  eta: 0:00:08  loss_g: -2.2225 (-2.1965)  loss_d: -0.0447 (-0.0622)  loss: -2.2927 (-2.2587)  time: 0.0270  data: 0.0000\n",
      "Epoch: [4]  [100/349]  eta: 0:00:06  loss_g: -2.5288 (-2.3357)  loss_d: -0.0995 (-0.0393)  loss: -2.5986 (-2.3750)  time: 0.0268  data: 0.0000\n",
      "Epoch: [4]  [150/349]  eta: 0:00:05  loss_g: -2.4979 (-2.3964)  loss_d: -0.0414 (-0.0487)  loss: -2.5393 (-2.4451)  time: 0.0278  data: 0.0000\n",
      "Epoch: [4]  [200/349]  eta: 0:00:04  loss_g: -2.4557 (-2.4133)  loss_d: -0.0009 (-0.0459)  loss: -2.4495 (-2.4592)  time: 0.0264  data: 0.0000\n",
      "Epoch: [4]  [250/349]  eta: 0:00:02  loss_g: -2.5381 (-2.4434)  loss_d: 0.0512 (-0.0340)  loss: -2.4866 (-2.4774)  time: 0.0267  data: 0.0000\n",
      "Epoch: [4]  [300/349]  eta: 0:00:01  loss_g: -2.5648 (-2.4650)  loss_d: 0.0985 (-0.0271)  loss: -2.4821 (-2.4921)  time: 0.0267  data: 0.0000\n",
      "Epoch: [4]  [349/349]  eta: 0:00:00  loss_g: -2.5209 (-2.4567)  loss_d: 0.0038 (-0.0270)  loss: -2.5784 (-2.4837)  time: 0.0293  data: 0.0000\n",
      "Epoch: [4] Total time: 0:00:09\n",
      "Epoch: [5]  [  0/349]  eta: 0:00:13  loss_g: -2.6796 (-2.6796)  loss_d: -0.3306 (-0.3306)  loss: -3.0102 (-3.0102)  time: 0.0382  data: 0.0000\n",
      "Epoch: [5]  [ 50/349]  eta: 0:00:08  loss_g: -2.3851 (-2.5237)  loss_d: -0.0017 (0.0066)  loss: -2.3692 (-2.5171)  time: 0.0278  data: 0.0000\n",
      "Epoch: [5]  [100/349]  eta: 0:00:06  loss_g: -2.5287 (-2.4960)  loss_d: -0.0790 (-0.0202)  loss: -2.5998 (-2.5162)  time: 0.0278  data: 0.0000\n",
      "Epoch: [5]  [150/349]  eta: 0:00:05  loss_g: -2.4218 (-2.4443)  loss_d: 0.0157 (-0.0197)  loss: -2.3953 (-2.4640)  time: 0.0263  data: 0.0000\n",
      "Epoch: [5]  [200/349]  eta: 0:00:04  loss_g: -2.3128 (-2.4254)  loss_d: -0.0363 (-0.0343)  loss: -2.3748 (-2.4597)  time: 0.0262  data: 0.0000\n",
      "Epoch: [5]  [250/349]  eta: 0:00:02  loss_g: -2.4913 (-2.4271)  loss_d: -0.0412 (-0.0322)  loss: -2.5155 (-2.4593)  time: 0.0263  data: 0.0000\n",
      "Epoch: [5]  [300/349]  eta: 0:00:01  loss_g: -2.4375 (-2.4375)  loss_d: 0.0403 (-0.0267)  loss: -2.4023 (-2.4641)  time: 0.0265  data: 0.0000\n",
      "Epoch: [5]  [349/349]  eta: 0:00:00  loss_g: -2.1926 (-2.4070)  loss_d: -0.0211 (-0.0251)  loss: -2.2225 (-2.4321)  time: 0.0265  data: 0.0000\n",
      "Epoch: [5] Total time: 0:00:09\n",
      "Epoch: [6]  [  0/349]  eta: 0:00:09  loss_g: -1.9544 (-1.9544)  loss_d: 0.2676 (0.2676)  loss: -1.6868 (-1.6868)  time: 0.0266  data: 0.0000\n",
      "Epoch: [6]  [ 50/349]  eta: 0:00:08  loss_g: -2.4044 (-2.4480)  loss_d: 0.0138 (-0.0425)  loss: -2.3861 (-2.4905)  time: 0.0274  data: 0.0000\n",
      "Epoch: [6]  [100/349]  eta: 0:00:06  loss_g: -2.2781 (-2.3539)  loss_d: -0.0622 (-0.0359)  loss: -2.2822 (-2.3898)  time: 0.0275  data: 0.0000\n",
      "Epoch: [6]  [150/349]  eta: 0:00:05  loss_g: -2.4378 (-2.3654)  loss_d: -0.0231 (-0.0302)  loss: -2.4866 (-2.3956)  time: 0.0262  data: 0.0000\n",
      "Epoch: [6]  [200/349]  eta: 0:00:04  loss_g: -2.3715 (-2.3734)  loss_d: -0.0083 (-0.0261)  loss: -2.3532 (-2.3995)  time: 0.0268  data: 0.0000\n",
      "Epoch: [6]  [250/349]  eta: 0:00:02  loss_g: -2.6012 (-2.4172)  loss_d: -0.0551 (-0.0168)  loss: -2.7232 (-2.4340)  time: 0.0267  data: 0.0000\n",
      "Epoch: [6]  [300/349]  eta: 0:00:01  loss_g: -2.3945 (-2.4027)  loss_d: 0.0067 (-0.0192)  loss: -2.4460 (-2.4219)  time: 0.0278  data: 0.0000\n",
      "Epoch: [6]  [349/349]  eta: 0:00:00  loss_g: -2.5842 (-2.4280)  loss_d: -0.0186 (-0.0193)  loss: -2.5933 (-2.4474)  time: 0.0266  data: 0.0000\n",
      "Epoch: [6] Total time: 0:00:09\n",
      "Epoch: [7]  [  0/349]  eta: 0:00:08  loss_g: -2.5155 (-2.5155)  loss_d: 0.1330 (0.1330)  loss: -2.3825 (-2.3825)  time: 0.0257  data: 0.0000\n",
      "Epoch: [7]  [ 50/349]  eta: 0:00:08  loss_g: -2.2996 (-2.3859)  loss_d: -0.2006 (-0.0675)  loss: -2.3810 (-2.4534)  time: 0.0261  data: 0.0000\n",
      "Epoch: [7]  [100/349]  eta: 0:00:06  loss_g: -2.4827 (-2.4114)  loss_d: -0.1087 (-0.0511)  loss: -2.5715 (-2.4625)  time: 0.0273  data: 0.0000\n",
      "Epoch: [7]  [150/349]  eta: 0:00:05  loss_g: -2.4718 (-2.4542)  loss_d: -0.1757 (-0.0566)  loss: -2.6421 (-2.5108)  time: 0.0273  data: 0.0000\n",
      "Epoch: [7]  [200/349]  eta: 0:00:04  loss_g: -2.3863 (-2.4146)  loss_d: -0.0122 (-0.0654)  loss: -2.4226 (-2.4801)  time: 0.0266  data: 0.0000\n",
      "Epoch: [7]  [250/349]  eta: 0:00:02  loss_g: -2.3460 (-2.4378)  loss_d: -0.0590 (-0.0606)  loss: -2.4378 (-2.4984)  time: 0.0280  data: 0.0000\n",
      "Epoch: [7]  [300/349]  eta: 0:00:01  loss_g: -2.2110 (-2.3905)  loss_d: -0.0631 (-0.0630)  loss: -2.3200 (-2.4536)  time: 0.0273  data: 0.0000\n",
      "Epoch: [7]  [349/349]  eta: 0:00:00  loss_g: -2.5637 (-2.4071)  loss_d: 0.0574 (-0.0614)  loss: -2.5442 (-2.4684)  time: 0.0275  data: 0.0000\n",
      "Epoch: [7] Total time: 0:00:09\n",
      "Epoch: [8]  [  0/349]  eta: 0:00:09  loss_g: -2.9039 (-2.9039)  loss_d: -0.0842 (-0.0842)  loss: -2.9880 (-2.9880)  time: 0.0275  data: 0.0000\n",
      "Epoch: [8]  [ 50/349]  eta: 0:00:08  loss_g: -2.1905 (-2.3049)  loss_d: -0.0912 (-0.0758)  loss: -2.3123 (-2.3807)  time: 0.0283  data: 0.0000\n",
      "Epoch: [8]  [100/349]  eta: 0:00:07  loss_g: -2.6021 (-2.3970)  loss_d: -0.0110 (-0.0692)  loss: -2.6203 (-2.4662)  time: 0.0286  data: 0.0000\n",
      "Epoch: [8]  [150/349]  eta: 0:00:05  loss_g: -2.3473 (-2.3496)  loss_d: -0.0584 (-0.0626)  loss: -2.4526 (-2.4122)  time: 0.0271  data: 0.0000\n",
      "Epoch: [8]  [200/349]  eta: 0:00:04  loss_g: -2.4616 (-2.3904)  loss_d: 0.0314 (-0.0539)  loss: -2.4533 (-2.4442)  time: 0.0271  data: 0.0000\n",
      "Epoch: [8]  [250/349]  eta: 0:00:02  loss_g: -2.3764 (-2.3815)  loss_d: -0.0089 (-0.0620)  loss: -2.3370 (-2.4435)  time: 0.0324  data: 0.0000\n",
      "Epoch: [8]  [300/349]  eta: 0:00:01  loss_g: -2.3592 (-2.3809)  loss_d: -0.1065 (-0.0579)  loss: -2.5001 (-2.4388)  time: 0.0275  data: 0.0000\n",
      "Epoch: [8]  [349/349]  eta: 0:00:00  loss_g: -2.6003 (-2.4091)  loss_d: -0.1827 (-0.0593)  loss: -2.7371 (-2.4684)  time: 0.0269  data: 0.0000\n",
      "Epoch: [8] Total time: 0:00:09\n",
      "Epoch: [9]  [  0/349]  eta: 0:00:09  loss_g: -2.8874 (-2.8874)  loss_d: -0.1093 (-0.1093)  loss: -2.9967 (-2.9967)  time: 0.0259  data: 0.0000\n",
      "Epoch: [9]  [ 50/349]  eta: 0:00:08  loss_g: -2.3038 (-2.3132)  loss_d: -0.0370 (-0.0629)  loss: -2.3366 (-2.3762)  time: 0.0270  data: 0.0000\n",
      "Epoch: [9]  [100/349]  eta: 0:00:06  loss_g: -2.2888 (-2.3492)  loss_d: -0.0567 (-0.0664)  loss: -2.3879 (-2.4157)  time: 0.0263  data: 0.0000\n",
      "Epoch: [9]  [150/349]  eta: 0:00:05  loss_g: -2.6011 (-2.3691)  loss_d: -0.0763 (-0.0519)  loss: -2.6661 (-2.4210)  time: 0.0290  data: 0.0000\n",
      "Epoch: [9]  [200/349]  eta: 0:00:04  loss_g: -2.0940 (-2.3380)  loss_d: -0.0548 (-0.0504)  loss: -2.1272 (-2.3884)  time: 0.0266  data: 0.0000\n",
      "Epoch: [9]  [250/349]  eta: 0:00:02  loss_g: -2.2655 (-2.3122)  loss_d: -0.0122 (-0.0531)  loss: -2.3035 (-2.3652)  time: 0.0270  data: 0.0000\n",
      "Epoch: [9]  [300/349]  eta: 0:00:01  loss_g: -2.2642 (-2.2890)  loss_d: -0.0376 (-0.0554)  loss: -2.3784 (-2.3444)  time: 0.0300  data: 0.0000\n",
      "Epoch: [9]  [349/349]  eta: 0:00:00  loss_g: -2.2358 (-2.3025)  loss_d: 0.0117 (-0.0481)  loss: -2.1750 (-2.3506)  time: 0.0301  data: 0.0000\n",
      "Epoch: [9] Total time: 0:00:09\n",
      "Epoch: [10]  [  0/349]  eta: 0:00:09  loss_g: -1.8934 (-1.8934)  loss_d: -0.1717 (-0.1717)  loss: -2.0651 (-2.0651)  time: 0.0282  data: 0.0000\n",
      "Epoch: [10]  [ 50/349]  eta: 0:00:08  loss_g: -2.3205 (-2.1929)  loss_d: -0.1254 (-0.0755)  loss: -2.4687 (-2.2683)  time: 0.0264  data: 0.0000\n",
      "Epoch: [10]  [100/349]  eta: 0:00:06  loss_g: -2.1666 (-2.2531)  loss_d: -0.0737 (-0.0865)  loss: -2.3590 (-2.3395)  time: 0.0274  data: 0.0000\n",
      "Epoch: [10]  [150/349]  eta: 0:00:05  loss_g: -2.0956 (-2.2089)  loss_d: -0.0782 (-0.0794)  loss: -2.2314 (-2.2882)  time: 0.0264  data: 0.0000\n",
      "Epoch: [10]  [200/349]  eta: 0:00:04  loss_g: -2.1218 (-2.1721)  loss_d: -0.1177 (-0.0825)  loss: -2.1959 (-2.2546)  time: 0.0269  data: 0.0000\n",
      "Epoch: [10]  [250/349]  eta: 0:00:02  loss_g: -1.9626 (-2.1651)  loss_d: -0.1119 (-0.0756)  loss: -2.0802 (-2.2407)  time: 0.0268  data: 0.0000\n",
      "Epoch: [10]  [300/349]  eta: 0:00:01  loss_g: -1.8840 (-2.1131)  loss_d: -0.0313 (-0.0694)  loss: -1.9597 (-2.1825)  time: 0.0272  data: 0.0000\n",
      "Epoch: [10]  [349/349]  eta: 0:00:00  loss_g: -2.1831 (-2.1249)  loss_d: -0.0875 (-0.0630)  loss: -2.2404 (-2.1879)  time: 0.0265  data: 0.0000\n",
      "Epoch: [10] Total time: 0:00:09\n",
      "Epoch: [11]  [  0/349]  eta: 0:00:10  loss_g: -1.7514 (-1.7514)  loss_d: -0.1493 (-0.1493)  loss: -1.9007 (-1.9007)  time: 0.0303  data: 0.0000\n",
      "Epoch: [11]  [ 50/349]  eta: 0:00:08  loss_g: -1.8696 (-1.7760)  loss_d: 0.0174 (-0.0297)  loss: -1.8165 (-1.8057)  time: 0.0274  data: 0.0000\n",
      "Epoch: [11]  [100/349]  eta: 0:00:06  loss_g: -1.9975 (-1.8715)  loss_d: -0.0844 (-0.0326)  loss: -2.0290 (-1.9041)  time: 0.0272  data: 0.0000\n",
      "Epoch: [11]  [150/349]  eta: 0:00:05  loss_g: -1.9945 (-1.9152)  loss_d: 0.0077 (-0.0347)  loss: -2.0727 (-1.9499)  time: 0.0268  data: 0.0000\n",
      "Epoch: [11]  [200/349]  eta: 0:00:04  loss_g: -1.9026 (-1.8932)  loss_d: -0.0941 (-0.0584)  loss: -2.0394 (-1.9516)  time: 0.0282  data: 0.0000\n",
      "Epoch: [11]  [250/349]  eta: 0:00:02  loss_g: -1.6299 (-1.8585)  loss_d: -0.0366 (-0.0599)  loss: -1.5599 (-1.9184)  time: 0.0268  data: 0.0000\n",
      "Epoch: [11]  [300/349]  eta: 0:00:01  loss_g: -1.6904 (-1.8102)  loss_d: -0.0204 (-0.0512)  loss: -1.6610 (-1.8614)  time: 0.0283  data: 0.0000\n",
      "Epoch: [11]  [349/349]  eta: 0:00:00  loss_g: -1.6136 (-1.7937)  loss_d: -0.1386 (-0.0532)  loss: -1.7289 (-1.8468)  time: 0.0275  data: 0.0000\n",
      "Epoch: [11] Total time: 0:00:09\n",
      "Epoch: [12]  [  0/349]  eta: 0:00:08  loss_g: -1.7502 (-1.7502)  loss_d: -0.1148 (-0.1148)  loss: -1.8650 (-1.8650)  time: 0.0254  data: 0.0000\n",
      "Epoch: [12]  [ 50/349]  eta: 0:00:08  loss_g: -1.4628 (-1.5928)  loss_d: -0.0184 (-0.0582)  loss: -1.4711 (-1.6511)  time: 0.0271  data: 0.0000\n",
      "Epoch: [12]  [100/349]  eta: 0:00:06  loss_g: -1.4239 (-1.4536)  loss_d: -0.0764 (-0.0530)  loss: -1.4897 (-1.5066)  time: 0.0276  data: 0.0000\n",
      "Epoch: [12]  [150/349]  eta: 0:00:05  loss_g: -1.5586 (-1.4907)  loss_d: 0.1046 (-0.0482)  loss: -1.5955 (-1.5390)  time: 0.0273  data: 0.0000\n",
      "Epoch: [12]  [200/349]  eta: 0:00:04  loss_g: -1.3689 (-1.4870)  loss_d: -0.0716 (-0.0504)  loss: -1.3658 (-1.5374)  time: 0.0272  data: 0.0000\n",
      "Epoch: [12]  [250/349]  eta: 0:00:02  loss_g: -1.6575 (-1.4831)  loss_d: -0.1725 (-0.0548)  loss: -1.7442 (-1.5379)  time: 0.0272  data: 0.0000\n",
      "Epoch: [12]  [300/349]  eta: 0:00:01  loss_g: -1.2495 (-1.4573)  loss_d: -0.0359 (-0.0538)  loss: -1.3292 (-1.5111)  time: 0.0280  data: 0.0000\n",
      "Epoch: [12]  [349/349]  eta: 0:00:00  loss_g: -1.0976 (-1.4083)  loss_d: 0.0149 (-0.0528)  loss: -1.2084 (-1.4611)  time: 0.0266  data: 0.0000\n",
      "Epoch: [12] Total time: 0:00:09\n",
      "Epoch: [13]  [  0/349]  eta: 0:00:09  loss_g: -1.0772 (-1.0772)  loss_d: -0.1154 (-0.1154)  loss: -1.1926 (-1.1926)  time: 0.0268  data: 0.0000\n",
      "Epoch: [13]  [ 50/349]  eta: 0:00:08  loss_g: -1.1969 (-1.2277)  loss_d: 0.0107 (-0.0012)  loss: -1.2156 (-1.2289)  time: 0.0269  data: 0.0000\n",
      "Epoch: [13]  [100/349]  eta: 0:00:06  loss_g: -1.1333 (-1.1497)  loss_d: 0.0483 (-0.0160)  loss: -1.0953 (-1.1656)  time: 0.0280  data: 0.0000\n",
      "Epoch: [13]  [150/349]  eta: 0:00:05  loss_g: -1.4543 (-1.2420)  loss_d: 0.0545 (-0.0035)  loss: -1.3675 (-1.2456)  time: 0.0270  data: 0.0000\n",
      "Epoch: [13]  [200/349]  eta: 0:00:04  loss_g: -1.3488 (-1.2863)  loss_d: -0.1462 (-0.0127)  loss: -1.3607 (-1.2990)  time: 0.0268  data: 0.0000\n",
      "Epoch: [13]  [250/349]  eta: 0:00:02  loss_g: -1.8150 (-1.3512)  loss_d: -0.0861 (-0.0149)  loss: -1.8695 (-1.3660)  time: 0.0260  data: 0.0000\n",
      "Epoch: [13]  [300/349]  eta: 0:00:01  loss_g: -1.3584 (-1.3964)  loss_d: 0.0215 (-0.0123)  loss: -1.4248 (-1.4087)  time: 0.0270  data: 0.0000\n",
      "Epoch: [13]  [349/349]  eta: 0:00:00  loss_g: -1.6419 (-1.3996)  loss_d: -0.0275 (-0.0097)  loss: -1.6589 (-1.4093)  time: 0.0277  data: 0.0000\n",
      "Epoch: [13] Total time: 0:00:09\n",
      "Epoch: [14]  [  0/349]  eta: 0:00:09  loss_g: -1.7438 (-1.7438)  loss_d: 0.1129 (0.1129)  loss: -1.6310 (-1.6310)  time: 0.0282  data: 0.0000\n",
      "Epoch: [14]  [ 50/349]  eta: 0:00:08  loss_g: -1.6792 (-1.7406)  loss_d: 0.0105 (0.0079)  loss: -1.6348 (-1.7327)  time: 0.0276  data: 0.0000\n",
      "Epoch: [14]  [100/349]  eta: 0:00:06  loss_g: -1.3596 (-1.6312)  loss_d: -0.0365 (-0.0133)  loss: -1.4011 (-1.6446)  time: 0.0268  data: 0.0000\n",
      "Epoch: [14]  [150/349]  eta: 0:00:05  loss_g: -1.6051 (-1.5576)  loss_d: 0.0758 (-0.0146)  loss: -1.5844 (-1.5722)  time: 0.0268  data: 0.0000\n",
      "Epoch: [14]  [200/349]  eta: 0:00:04  loss_g: -1.5139 (-1.5238)  loss_d: 0.0535 (-0.0225)  loss: -1.5584 (-1.5463)  time: 0.0272  data: 0.0000\n",
      "Epoch: [14]  [250/349]  eta: 0:00:02  loss_g: -1.2894 (-1.4971)  loss_d: 0.0141 (-0.0252)  loss: -1.3670 (-1.5223)  time: 0.0275  data: 0.0000\n",
      "Epoch: [14]  [300/349]  eta: 0:00:01  loss_g: -1.5379 (-1.5031)  loss_d: 0.0488 (-0.0208)  loss: -1.4647 (-1.5239)  time: 0.0280  data: 0.0000\n",
      "Epoch: [14]  [349/349]  eta: 0:00:00  loss_g: -1.4246 (-1.4852)  loss_d: 0.0163 (-0.0226)  loss: -1.2823 (-1.5079)  time: 0.0266  data: 0.0000\n",
      "Epoch: [14] Total time: 0:00:09\n",
      "Epoch: [15]  [  0/349]  eta: 0:00:08  loss_g: -1.5314 (-1.5314)  loss_d: -0.6396 (-0.6396)  loss: -2.1710 (-2.1710)  time: 0.0255  data: 0.0000\n",
      "Epoch: [15]  [ 50/349]  eta: 0:00:07  loss_g: -1.3291 (-1.3567)  loss_d: 0.0337 (0.0148)  loss: -1.2691 (-1.3419)  time: 0.0261  data: 0.0000\n",
      "Epoch: [15]  [100/349]  eta: 0:00:06  loss_g: -1.6610 (-1.5102)  loss_d: -0.2218 (-0.0176)  loss: -1.9696 (-1.5278)  time: 0.0296  data: 0.0000\n",
      "Epoch: [15]  [150/349]  eta: 0:00:05  loss_g: -1.6359 (-1.5211)  loss_d: -0.0489 (-0.0148)  loss: -1.6577 (-1.5359)  time: 0.0270  data: 0.0000\n",
      "Epoch: [15]  [200/349]  eta: 0:00:04  loss_g: -1.7341 (-1.5418)  loss_d: 0.0549 (-0.0146)  loss: -1.6445 (-1.5564)  time: 0.0266  data: 0.0000\n",
      "Epoch: [15]  [250/349]  eta: 0:00:02  loss_g: -1.6792 (-1.5769)  loss_d: -0.1311 (-0.0285)  loss: -1.8575 (-1.6054)  time: 0.0271  data: 0.0000\n",
      "Epoch: [15]  [300/349]  eta: 0:00:01  loss_g: -1.5747 (-1.5791)  loss_d: -0.1653 (-0.0437)  loss: -1.6928 (-1.6229)  time: 0.0270  data: 0.0000\n",
      "Epoch: [15]  [349/349]  eta: 0:00:00  loss_g: -1.3495 (-1.5446)  loss_d: 0.0103 (-0.0396)  loss: -1.2689 (-1.5842)  time: 0.0267  data: 0.0000\n",
      "Epoch: [15] Total time: 0:00:09\n",
      "Epoch: [16]  [  0/349]  eta: 0:00:08  loss_g: -1.3770 (-1.3770)  loss_d: -0.1117 (-0.1117)  loss: -1.4887 (-1.4887)  time: 0.0242  data: 0.0000\n",
      "Epoch: [16]  [ 50/349]  eta: 0:00:08  loss_g: -1.4567 (-1.5573)  loss_d: -0.0428 (-0.0661)  loss: -1.3979 (-1.6234)  time: 0.0277  data: 0.0000\n",
      "Epoch: [16]  [100/349]  eta: 0:00:07  loss_g: -1.1931 (-1.3404)  loss_d: -0.0004 (-0.0519)  loss: -1.2047 (-1.3922)  time: 0.0267  data: 0.0000\n",
      "Epoch: [16]  [150/349]  eta: 0:00:05  loss_g: -1.4213 (-1.3193)  loss_d: -0.1163 (-0.0371)  loss: -1.4889 (-1.3563)  time: 0.0272  data: 0.0000\n",
      "Epoch: [16]  [200/349]  eta: 0:00:04  loss_g: -1.1382 (-1.3320)  loss_d: -0.2870 (-0.0486)  loss: -1.4423 (-1.3806)  time: 0.0275  data: 0.0000\n",
      "Epoch: [16]  [250/349]  eta: 0:00:02  loss_g: -1.3934 (-1.2763)  loss_d: 0.0917 (-0.0422)  loss: -1.4044 (-1.3185)  time: 0.0281  data: 0.0000\n",
      "Epoch: [16]  [300/349]  eta: 0:00:01  loss_g: -1.6730 (-1.3391)  loss_d: -0.0283 (-0.0493)  loss: -1.5809 (-1.3884)  time: 0.0276  data: 0.0000\n",
      "Epoch: [16]  [349/349]  eta: 0:00:00  loss_g: -0.9132 (-1.3148)  loss_d: -0.0921 (-0.0484)  loss: -1.1024 (-1.3632)  time: 0.0265  data: 0.0000\n",
      "Epoch: [16] Total time: 0:00:09\n",
      "Epoch: [17]  [  0/349]  eta: 0:00:09  loss_g: -0.8238 (-0.8238)  loss_d: 0.5537 (0.5537)  loss: -0.2701 (-0.2701)  time: 0.0260  data: 0.0000\n",
      "Epoch: [17]  [ 50/349]  eta: 0:00:08  loss_g: -1.1163 (-1.0351)  loss_d: -0.2515 (-0.1037)  loss: -1.2982 (-1.1388)  time: 0.0271  data: 0.0000\n",
      "Epoch: [17]  [100/349]  eta: 0:00:06  loss_g: -1.1703 (-1.1032)  loss_d: 0.0115 (-0.0620)  loss: -1.0752 (-1.1651)  time: 0.0270  data: 0.0000\n",
      "Epoch: [17]  [150/349]  eta: 0:00:05  loss_g: -1.0726 (-1.0613)  loss_d: -0.0762 (-0.0289)  loss: -0.9659 (-1.0902)  time: 0.0272  data: 0.0000\n",
      "Epoch: [17]  [200/349]  eta: 0:00:04  loss_g: -1.5981 (-1.1173)  loss_d: -0.0080 (-0.0093)  loss: -1.5649 (-1.1266)  time: 0.0274  data: 0.0000\n",
      "Epoch: [17]  [250/349]  eta: 0:00:02  loss_g: -1.5766 (-1.2006)  loss_d: -0.0590 (-0.0111)  loss: -1.4403 (-1.2118)  time: 0.0286  data: 0.0000\n",
      "Epoch: [17]  [300/349]  eta: 0:00:01  loss_g: -1.7589 (-1.3017)  loss_d: 0.0065 (-0.0185)  loss: -1.8290 (-1.3202)  time: 0.0286  data: 0.0000\n",
      "Epoch: [17]  [349/349]  eta: 0:00:00  loss_g: -1.6377 (-1.3342)  loss_d: 0.0615 (-0.0206)  loss: -1.5867 (-1.3548)  time: 0.0274  data: 0.0000\n",
      "Epoch: [17] Total time: 0:00:09\n",
      "Epoch: [18]  [  0/349]  eta: 0:00:08  loss_g: -1.4243 (-1.4243)  loss_d: -0.1815 (-0.1815)  loss: -1.6058 (-1.6058)  time: 0.0257  data: 0.0000\n",
      "Epoch: [18]  [ 50/349]  eta: 0:00:08  loss_g: -1.6459 (-1.5956)  loss_d: -0.0500 (0.0093)  loss: -1.7522 (-1.5863)  time: 0.0275  data: 0.0000\n",
      "Epoch: [18]  [100/349]  eta: 0:00:06  loss_g: -1.7208 (-1.6672)  loss_d: 0.1605 (0.0054)  loss: -1.6269 (-1.6617)  time: 0.0264  data: 0.0000\n",
      "Epoch: [18]  [150/349]  eta: 0:00:05  loss_g: -1.5462 (-1.6481)  loss_d: 0.0601 (0.0017)  loss: -1.2939 (-1.6464)  time: 0.0271  data: 0.0000\n",
      "Epoch: [18]  [200/349]  eta: 0:00:04  loss_g: -1.4404 (-1.5862)  loss_d: -0.0389 (-0.0195)  loss: -1.4824 (-1.6057)  time: 0.0274  data: 0.0000\n",
      "Epoch: [18]  [250/349]  eta: 0:00:02  loss_g: -1.6580 (-1.5847)  loss_d: 0.0969 (-0.0001)  loss: -1.5760 (-1.5849)  time: 0.0277  data: 0.0000\n",
      "Epoch: [18]  [300/349]  eta: 0:00:01  loss_g: -1.8070 (-1.6090)  loss_d: -0.0190 (-0.0054)  loss: -1.8215 (-1.6144)  time: 0.0279  data: 0.0000\n",
      "Epoch: [18]  [349/349]  eta: 0:00:00  loss_g: -1.4019 (-1.6011)  loss_d: -0.0634 (-0.0092)  loss: -1.3721 (-1.6103)  time: 0.0273  data: 0.0000\n",
      "Epoch: [18] Total time: 0:00:09\n",
      "Epoch: [19]  [  0/349]  eta: 0:00:09  loss_g: -1.3309 (-1.3309)  loss_d: -0.4213 (-0.4213)  loss: -1.7522 (-1.7522)  time: 0.0267  data: 0.0000\n",
      "Epoch: [19]  [ 50/349]  eta: 0:00:08  loss_g: -1.6876 (-1.6026)  loss_d: -0.0547 (0.0095)  loss: -1.7894 (-1.5931)  time: 0.0267  data: 0.0000\n",
      "Epoch: [19]  [100/349]  eta: 0:00:06  loss_g: -1.7785 (-1.7496)  loss_d: -0.0599 (-0.0470)  loss: -1.9209 (-1.7965)  time: 0.0274  data: 0.0000\n",
      "Epoch: [19]  [150/349]  eta: 0:00:05  loss_g: -1.5495 (-1.6232)  loss_d: -0.0309 (-0.0351)  loss: -1.5064 (-1.6583)  time: 0.0272  data: 0.0000\n",
      "Epoch: [19]  [200/349]  eta: 0:00:04  loss_g: -1.8459 (-1.6416)  loss_d: 0.0561 (-0.0411)  loss: -1.8450 (-1.6827)  time: 0.0279  data: 0.0000\n",
      "Epoch: [19]  [250/349]  eta: 0:00:02  loss_g: -2.0007 (-1.7112)  loss_d: 0.1407 (-0.0148)  loss: -2.0156 (-1.7260)  time: 0.0265  data: 0.0000\n",
      "Epoch: [19]  [300/349]  eta: 0:00:01  loss_g: -1.4838 (-1.6765)  loss_d: -0.0059 (-0.0263)  loss: -1.4042 (-1.7028)  time: 0.0263  data: 0.0000\n",
      "Epoch: [19]  [349/349]  eta: 0:00:00  loss_g: -1.9934 (-1.7227)  loss_d: -0.2084 (-0.0399)  loss: -2.2312 (-1.7625)  time: 0.0289  data: 0.0000\n",
      "Epoch: [19] Total time: 0:00:09\n",
      "Epoch: [20]  [  0/349]  eta: 0:00:08  loss_g: -2.2512 (-2.2512)  loss_d: 0.4431 (0.4431)  loss: -1.8081 (-1.8081)  time: 0.0250  data: 0.0000\n",
      "Epoch: [20]  [ 50/349]  eta: 0:00:08  loss_g: -1.7345 (-1.8246)  loss_d: -0.0452 (-0.0199)  loss: -1.7215 (-1.8445)  time: 0.0276  data: 0.0000\n",
      "Epoch: [20]  [100/349]  eta: 0:00:06  loss_g: -1.6516 (-1.7004)  loss_d: -0.0344 (-0.0227)  loss: -1.7295 (-1.7231)  time: 0.0264  data: 0.0000\n",
      "Epoch: [20]  [150/349]  eta: 0:00:05  loss_g: -1.8778 (-1.6982)  loss_d: -0.0405 (-0.0246)  loss: -1.8060 (-1.7229)  time: 0.0278  data: 0.0000\n",
      "Epoch: [20]  [200/349]  eta: 0:00:04  loss_g: -1.8231 (-1.7212)  loss_d: 0.1678 (-0.0202)  loss: -1.6855 (-1.7414)  time: 0.0270  data: 0.0000\n",
      "Epoch: [20]  [250/349]  eta: 0:00:02  loss_g: -1.3114 (-1.6820)  loss_d: -0.1098 (-0.0223)  loss: -1.3810 (-1.7043)  time: 0.0269  data: 0.0000\n",
      "Epoch: [20]  [300/349]  eta: 0:00:01  loss_g: -1.8307 (-1.6787)  loss_d: 0.1444 (-0.0130)  loss: -1.6554 (-1.6917)  time: 0.0270  data: 0.0000\n",
      "Epoch: [20]  [349/349]  eta: 0:00:00  loss_g: -1.4544 (-1.6631)  loss_d: -0.0635 (-0.0234)  loss: -1.4992 (-1.6865)  time: 0.0261  data: 0.0000\n",
      "Epoch: [20] Total time: 0:00:09\n",
      "Epoch: [21]  [  0/349]  eta: 0:00:10  loss_g: -1.8379 (-1.8379)  loss_d: 0.1112 (0.1112)  loss: -1.7267 (-1.7267)  time: 0.0301  data: 0.0000\n",
      "Epoch: [21]  [ 50/349]  eta: 0:00:08  loss_g: -1.6385 (-1.4960)  loss_d: -0.0630 (-0.0761)  loss: -1.7158 (-1.5721)  time: 0.0293  data: 0.0000\n",
      "Epoch: [21]  [100/349]  eta: 0:00:07  loss_g: -1.6664 (-1.6155)  loss_d: 0.0452 (-0.0221)  loss: -1.5160 (-1.6375)  time: 0.0289  data: 0.0000\n",
      "Epoch: [21]  [150/349]  eta: 0:00:05  loss_g: -1.5325 (-1.5644)  loss_d: -0.0339 (-0.0138)  loss: -1.4790 (-1.5782)  time: 0.0269  data: 0.0000\n",
      "Epoch: [21]  [200/349]  eta: 0:00:04  loss_g: -1.9856 (-1.6167)  loss_d: 0.0899 (0.0064)  loss: -1.7767 (-1.6103)  time: 0.0265  data: 0.0000\n",
      "Epoch: [21]  [250/349]  eta: 0:00:02  loss_g: -1.8086 (-1.6640)  loss_d: -0.1992 (-0.0066)  loss: -1.9450 (-1.6707)  time: 0.0282  data: 0.0000\n",
      "Epoch: [21]  [300/349]  eta: 0:00:01  loss_g: -2.1046 (-1.7382)  loss_d: -0.1133 (-0.0190)  loss: -2.2622 (-1.7572)  time: 0.0275  data: 0.0000\n",
      "Epoch: [21]  [349/349]  eta: 0:00:00  loss_g: -1.8771 (-1.7449)  loss_d: -0.0964 (-0.0282)  loss: -2.0079 (-1.7731)  time: 0.0269  data: 0.0000\n",
      "Epoch: [21] Total time: 0:00:09\n",
      "Epoch: [22]  [  0/349]  eta: 0:00:09  loss_g: -1.8733 (-1.8733)  loss_d: 0.0564 (0.0564)  loss: -1.8169 (-1.8169)  time: 0.0263  data: 0.0000\n",
      "Epoch: [22]  [ 50/349]  eta: 0:00:08  loss_g: -2.0167 (-2.0294)  loss_d: 0.0201 (-0.0792)  loss: -2.0994 (-2.1086)  time: 0.0267  data: 0.0000\n",
      "Epoch: [22]  [100/349]  eta: 0:00:06  loss_g: -1.5978 (-1.9693)  loss_d: -0.1723 (-0.0666)  loss: -1.7579 (-2.0359)  time: 0.0262  data: 0.0000\n",
      "Epoch: [22]  [150/349]  eta: 0:00:05  loss_g: -2.0417 (-1.8933)  loss_d: -0.0786 (-0.0474)  loss: -2.1387 (-1.9407)  time: 0.0297  data: 0.0000\n",
      "Epoch: [22]  [200/349]  eta: 0:00:04  loss_g: -1.6418 (-1.8868)  loss_d: -0.0068 (-0.0572)  loss: -1.4595 (-1.9440)  time: 0.0267  data: 0.0000\n",
      "Epoch: [22]  [250/349]  eta: 0:00:02  loss_g: -1.7802 (-1.8156)  loss_d: -0.0233 (-0.0334)  loss: -1.6598 (-1.8489)  time: 0.0265  data: 0.0000\n",
      "Epoch: [22]  [300/349]  eta: 0:00:01  loss_g: -1.7348 (-1.7973)  loss_d: -0.0192 (-0.0418)  loss: -1.6989 (-1.8390)  time: 0.0259  data: 0.0000\n",
      "Epoch: [22]  [349/349]  eta: 0:00:00  loss_g: -1.7523 (-1.7964)  loss_d: -0.0668 (-0.0361)  loss: -1.8519 (-1.8326)  time: 0.0261  data: 0.0000\n",
      "Epoch: [22] Total time: 0:00:09\n",
      "Epoch: [23]  [  0/349]  eta: 0:00:09  loss_g: -1.8392 (-1.8392)  loss_d: -0.1415 (-0.1415)  loss: -1.9807 (-1.9807)  time: 0.0260  data: 0.0000\n",
      "Epoch: [23]  [ 50/349]  eta: 0:00:07  loss_g: -1.5205 (-1.4444)  loss_d: -0.1504 (0.0350)  loss: -1.4921 (-1.4094)  time: 0.0262  data: 0.0000\n",
      "Epoch: [23]  [100/349]  eta: 0:00:06  loss_g: -1.8951 (-1.6831)  loss_d: -0.1300 (-0.0344)  loss: -1.9573 (-1.7175)  time: 0.0259  data: 0.0000\n",
      "Epoch: [23]  [150/349]  eta: 0:00:05  loss_g: -1.3976 (-1.6238)  loss_d: 0.0088 (-0.0207)  loss: -1.3279 (-1.6445)  time: 0.0265  data: 0.0000\n",
      "Epoch: [23]  [200/349]  eta: 0:00:03  loss_g: -1.7594 (-1.6210)  loss_d: 0.0324 (-0.0257)  loss: -1.9930 (-1.6468)  time: 0.0270  data: 0.0000\n",
      "Epoch: [23]  [250/349]  eta: 0:00:02  loss_g: -1.7488 (-1.6389)  loss_d: -0.2323 (-0.0465)  loss: -1.9909 (-1.6853)  time: 0.0291  data: 0.0000\n",
      "Epoch: [23]  [300/349]  eta: 0:00:01  loss_g: -1.7024 (-1.6401)  loss_d: -0.0035 (-0.0374)  loss: -1.6878 (-1.6775)  time: 0.0253  data: 0.0000\n",
      "Epoch: [23]  [349/349]  eta: 0:00:00  loss_g: -2.0767 (-1.6878)  loss_d: -0.0148 (-0.0340)  loss: -2.0361 (-1.7218)  time: 0.0269  data: 0.0000\n",
      "Epoch: [23] Total time: 0:00:09\n",
      "Epoch: [24]  [  0/349]  eta: 0:00:08  loss_g: -2.1390 (-2.1390)  loss_d: 0.2108 (0.2108)  loss: -1.9282 (-1.9282)  time: 0.0238  data: 0.0000\n",
      "Epoch: [24]  [ 50/349]  eta: 0:00:08  loss_g: -1.5824 (-1.5514)  loss_d: -0.0002 (-0.0345)  loss: -1.4650 (-1.5859)  time: 0.0272  data: 0.0000\n",
      "Epoch: [24]  [100/349]  eta: 0:00:06  loss_g: -1.9812 (-1.7482)  loss_d: -0.0865 (-0.0319)  loss: -2.0440 (-1.7801)  time: 0.0280  data: 0.0000\n",
      "Epoch: [24]  [150/349]  eta: 0:00:05  loss_g: -1.5918 (-1.7181)  loss_d: -0.0501 (-0.0548)  loss: -1.4789 (-1.7729)  time: 0.0261  data: 0.0000\n",
      "Epoch: [24]  [200/349]  eta: 0:00:04  loss_g: -1.7584 (-1.6981)  loss_d: -0.1366 (-0.0383)  loss: -1.7901 (-1.7363)  time: 0.0272  data: 0.0000\n",
      "Epoch: [24]  [250/349]  eta: 0:00:02  loss_g: -1.8937 (-1.7663)  loss_d: -0.0471 (-0.0398)  loss: -1.9073 (-1.8061)  time: 0.0263  data: 0.0000\n",
      "Epoch: [24]  [300/349]  eta: 0:00:01  loss_g: -1.5474 (-1.7274)  loss_d: -0.1321 (-0.0319)  loss: -1.6223 (-1.7593)  time: 0.0269  data: 0.0000\n",
      "Epoch: [24]  [349/349]  eta: 0:00:00  loss_g: -2.0931 (-1.7623)  loss_d: -0.1566 (-0.0408)  loss: -2.2584 (-1.8031)  time: 0.0267  data: 0.0000\n",
      "Epoch: [24] Total time: 0:00:09\n",
      "Epoch: [25]  [  0/349]  eta: 0:00:08  loss_g: -1.5065 (-1.5065)  loss_d: 0.0917 (0.0917)  loss: -1.4148 (-1.4148)  time: 0.0252  data: 0.0000\n",
      "Epoch: [25]  [ 50/349]  eta: 0:00:08  loss_g: -1.5567 (-1.6633)  loss_d: -0.2337 (-0.0589)  loss: -1.6502 (-1.7222)  time: 0.0271  data: 0.0000\n",
      "Epoch: [25]  [100/349]  eta: 0:00:06  loss_g: -1.5178 (-1.5873)  loss_d: -0.1102 (-0.0426)  loss: -1.5486 (-1.6299)  time: 0.0260  data: 0.0000\n",
      "Epoch: [25]  [150/349]  eta: 0:00:05  loss_g: -1.5195 (-1.5776)  loss_d: 0.0417 (-0.0253)  loss: -1.4722 (-1.6029)  time: 0.0272  data: 0.0000\n",
      "Epoch: [25]  [200/349]  eta: 0:00:04  loss_g: -1.6393 (-1.5711)  loss_d: 0.0009 (-0.0314)  loss: -1.5546 (-1.6025)  time: 0.0271  data: 0.0000\n",
      "Epoch: [25]  [250/349]  eta: 0:00:02  loss_g: -1.3147 (-1.5185)  loss_d: 0.0771 (-0.0347)  loss: -1.2030 (-1.5532)  time: 0.0267  data: 0.0000\n",
      "Epoch: [25]  [300/349]  eta: 0:00:01  loss_g: -1.3793 (-1.5016)  loss_d: -0.1149 (-0.0387)  loss: -1.5453 (-1.5403)  time: 0.0286  data: 0.0000\n",
      "Epoch: [25]  [349/349]  eta: 0:00:00  loss_g: -1.6225 (-1.4870)  loss_d: 0.0318 (-0.0389)  loss: -1.6065 (-1.5259)  time: 0.0294  data: 0.0000\n",
      "Epoch: [25] Total time: 0:00:09\n",
      "Epoch: [26]  [  0/349]  eta: 0:00:10  loss_g: -1.7192 (-1.7192)  loss_d: -0.6390 (-0.6390)  loss: -2.3582 (-2.3582)  time: 0.0288  data: 0.0000\n",
      "Epoch: [26]  [ 50/349]  eta: 0:00:08  loss_g: -1.7603 (-1.8038)  loss_d: -0.2626 (-0.0237)  loss: -2.0205 (-1.8275)  time: 0.0271  data: 0.0000\n",
      "Epoch: [26]  [100/349]  eta: 0:00:06  loss_g: -1.3841 (-1.6569)  loss_d: -0.1692 (-0.0263)  loss: -1.5047 (-1.6833)  time: 0.0266  data: 0.0000\n",
      "Epoch: [26]  [150/349]  eta: 0:00:05  loss_g: -1.9046 (-1.6928)  loss_d: -0.1123 (-0.0335)  loss: -1.9297 (-1.7264)  time: 0.0257  data: 0.0000\n",
      "Epoch: [26]  [200/349]  eta: 0:00:04  loss_g: -1.5594 (-1.6358)  loss_d: -0.2721 (-0.0551)  loss: -1.8199 (-1.6909)  time: 0.0272  data: 0.0000\n",
      "Epoch: [26]  [250/349]  eta: 0:00:02  loss_g: -1.7688 (-1.6063)  loss_d: -0.1115 (-0.0565)  loss: -1.8265 (-1.6629)  time: 0.0274  data: 0.0000\n",
      "Epoch: [26]  [300/349]  eta: 0:00:01  loss_g: -1.4396 (-1.6044)  loss_d: -0.0825 (-0.0547)  loss: -1.5187 (-1.6591)  time: 0.0264  data: 0.0000\n",
      "Epoch: [26]  [349/349]  eta: 0:00:00  loss_g: -1.7622 (-1.6071)  loss_d: -0.2116 (-0.0633)  loss: -2.0020 (-1.6704)  time: 0.0262  data: 0.0000\n",
      "Epoch: [26] Total time: 0:00:09\n",
      "Epoch: [27]  [  0/349]  eta: 0:00:08  loss_g: -1.8664 (-1.8664)  loss_d: -0.4022 (-0.4022)  loss: -2.2685 (-2.2685)  time: 0.0232  data: 0.0000\n",
      "Epoch: [27]  [ 50/349]  eta: 0:00:08  loss_g: -1.4133 (-1.4907)  loss_d: -0.2312 (-0.1320)  loss: -1.6059 (-1.6228)  time: 0.0270  data: 0.0000\n",
      "Epoch: [27]  [100/349]  eta: 0:00:06  loss_g: -1.5237 (-1.5026)  loss_d: 0.0059 (-0.0739)  loss: -1.6814 (-1.5765)  time: 0.0266  data: 0.0000\n",
      "Epoch: [27]  [150/349]  eta: 0:00:05  loss_g: -1.7265 (-1.5988)  loss_d: -0.2659 (-0.0769)  loss: -1.8079 (-1.6757)  time: 0.0295  data: 0.0000\n",
      "Epoch: [27]  [200/349]  eta: 0:00:04  loss_g: -1.1870 (-1.5077)  loss_d: -0.1262 (-0.0708)  loss: -1.3193 (-1.5785)  time: 0.0266  data: 0.0000\n",
      "Epoch: [27]  [250/349]  eta: 0:00:02  loss_g: -1.3857 (-1.4740)  loss_d: -0.3149 (-0.0825)  loss: -1.6813 (-1.5565)  time: 0.0272  data: 0.0000\n",
      "Epoch: [27]  [300/349]  eta: 0:00:01  loss_g: -1.0864 (-1.3903)  loss_d: -0.1319 (-0.0942)  loss: -1.4319 (-1.4845)  time: 0.0254  data: 0.0000\n",
      "Epoch: [27]  [349/349]  eta: 0:00:00  loss_g: -1.1213 (-1.3671)  loss_d: -0.3465 (-0.1017)  loss: -1.5293 (-1.4688)  time: 0.0267  data: 0.0000\n",
      "Epoch: [27] Total time: 0:00:09\n",
      "Epoch: [28]  [  0/349]  eta: 0:00:09  loss_g: -0.7870 (-0.7870)  loss_d: 0.1246 (0.1246)  loss: -0.6624 (-0.6624)  time: 0.0272  data: 0.0000\n",
      "Epoch: [28]  [ 50/349]  eta: 0:00:08  loss_g: -0.9043 (-1.1045)  loss_d: -0.0061 (0.0395)  loss: -0.8141 (-1.0649)  time: 0.0292  data: 0.0000\n",
      "Epoch: [28]  [100/349]  eta: 0:00:07  loss_g: -1.2121 (-1.0212)  loss_d: 0.0521 (0.0128)  loss: -1.2037 (-1.0084)  time: 0.0267  data: 0.0000\n",
      "Epoch: [28]  [150/349]  eta: 0:00:05  loss_g: -1.1814 (-1.1253)  loss_d: 0.1638 (0.0175)  loss: -1.0405 (-1.1079)  time: 0.0277  data: 0.0000\n",
      "Epoch: [28]  [200/349]  eta: 0:00:04  loss_g: -1.1134 (-1.0949)  loss_d: 0.0499 (0.0108)  loss: -1.1511 (-1.0842)  time: 0.0270  data: 0.0000\n",
      "Epoch: [28]  [250/349]  eta: 0:00:02  loss_g: -1.4225 (-1.1724)  loss_d: -0.1105 (-0.0008)  loss: -1.4808 (-1.1731)  time: 0.0269  data: 0.0000\n",
      "Epoch: [28]  [300/349]  eta: 0:00:01  loss_g: -1.4573 (-1.1947)  loss_d: -0.2267 (-0.0093)  loss: -1.5321 (-1.2041)  time: 0.0271  data: 0.0000\n",
      "Epoch: [28]  [349/349]  eta: 0:00:00  loss_g: -0.7548 (-1.1766)  loss_d: -0.1705 (-0.0141)  loss: -0.7939 (-1.1907)  time: 0.0270  data: 0.0000\n",
      "Epoch: [28] Total time: 0:00:09\n",
      "Epoch: [29]  [  0/349]  eta: 0:00:09  loss_g: -0.3143 (-0.3143)  loss_d: 0.1639 (0.1639)  loss: -0.1504 (-0.1504)  time: 0.0258  data: 0.0000\n",
      "Epoch: [29]  [ 50/349]  eta: 0:00:08  loss_g: -1.1034 (-0.8246)  loss_d: 0.0436 (0.0851)  loss: -0.8600 (-0.7394)  time: 0.0258  data: 0.0000\n",
      "Epoch: [29]  [100/349]  eta: 0:00:06  loss_g: -1.2820 (-1.0301)  loss_d: 0.0624 (0.0329)  loss: -1.2014 (-0.9972)  time: 0.0263  data: 0.0000\n",
      "Epoch: [29]  [150/349]  eta: 0:00:05  loss_g: -1.1405 (-1.0969)  loss_d: -0.1763 (0.0054)  loss: -1.2700 (-1.0916)  time: 0.0305  data: 0.0000\n",
      "Epoch: [29]  [200/349]  eta: 0:00:04  loss_g: -1.2205 (-1.1144)  loss_d: 0.0851 (0.0139)  loss: -1.1302 (-1.1005)  time: 0.0263  data: 0.0000\n",
      "Epoch: [29]  [250/349]  eta: 0:00:02  loss_g: -1.4154 (-1.1516)  loss_d: -0.1054 (0.0051)  loss: -1.5938 (-1.1465)  time: 0.0277  data: 0.0000\n",
      "Epoch: [29]  [300/349]  eta: 0:00:01  loss_g: -1.5648 (-1.2134)  loss_d: -0.0864 (0.0013)  loss: -1.6879 (-1.2120)  time: 0.0264  data: 0.0000\n",
      "Epoch: [29]  [349/349]  eta: 0:00:00  loss_g: -1.0666 (-1.2102)  loss_d: -0.1239 (-0.0138)  loss: -1.0830 (-1.2241)  time: 0.0270  data: 0.0000\n",
      "Epoch: [29] Total time: 0:00:09\n",
      "Epoch: [30]  [  0/349]  eta: 0:00:08  loss_g: -1.2047 (-1.2047)  loss_d: -0.0155 (-0.0155)  loss: -1.2201 (-1.2201)  time: 0.0256  data: 0.0000\n",
      "Epoch: [30]  [ 50/349]  eta: 0:00:07  loss_g: -1.5078 (-1.2887)  loss_d: -0.0806 (-0.1935)  loss: -1.6202 (-1.4822)  time: 0.0263  data: 0.0000\n",
      "Epoch: [30]  [100/349]  eta: 0:00:06  loss_g: -1.3037 (-1.3107)  loss_d: 0.0491 (-0.1115)  loss: -1.0612 (-1.4222)  time: 0.0270  data: 0.0000\n",
      "Epoch: [30]  [150/349]  eta: 0:00:05  loss_g: -1.3544 (-1.2673)  loss_d: -0.0857 (-0.0735)  loss: -1.3256 (-1.3408)  time: 0.0263  data: 0.0000\n",
      "Epoch: [30]  [200/349]  eta: 0:00:04  loss_g: -1.5376 (-1.3117)  loss_d: -0.0946 (-0.0545)  loss: -1.5877 (-1.3662)  time: 0.0269  data: 0.0000\n",
      "Epoch: [30]  [250/349]  eta: 0:00:02  loss_g: -1.5069 (-1.3673)  loss_d: -0.1732 (-0.0455)  loss: -1.5668 (-1.4128)  time: 0.0261  data: 0.0000\n",
      "Epoch: [30]  [300/349]  eta: 0:00:01  loss_g: -1.6175 (-1.4106)  loss_d: -0.0245 (-0.0400)  loss: -1.6670 (-1.4506)  time: 0.0267  data: 0.0000\n",
      "Epoch: [30]  [349/349]  eta: 0:00:00  loss_g: -1.1910 (-1.3566)  loss_d: -0.1778 (-0.0474)  loss: -1.3529 (-1.4039)  time: 0.0266  data: 0.0000\n",
      "Epoch: [30] Total time: 0:00:09\n",
      "Epoch: [31]  [  0/349]  eta: 0:00:08  loss_g: -1.2489 (-1.2489)  loss_d: 0.4564 (0.4564)  loss: -0.7925 (-0.7925)  time: 0.0250  data: 0.0000\n",
      "Epoch: [31]  [ 50/349]  eta: 0:00:07  loss_g: -1.8122 (-1.6894)  loss_d: -0.1756 (-0.1046)  loss: -1.7532 (-1.7940)  time: 0.0263  data: 0.0000\n",
      "Epoch: [31]  [100/349]  eta: 0:00:06  loss_g: -1.3474 (-1.5645)  loss_d: -0.1988 (-0.1103)  loss: -1.5372 (-1.6749)  time: 0.0274  data: 0.0000\n",
      "Epoch: [31]  [150/349]  eta: 0:00:05  loss_g: -1.5579 (-1.5630)  loss_d: -0.2453 (-0.1432)  loss: -1.7357 (-1.7062)  time: 0.0258  data: 0.0000\n",
      "Epoch: [31]  [200/349]  eta: 0:00:04  loss_g: -1.6512 (-1.5937)  loss_d: 0.1496 (-0.1018)  loss: -1.4030 (-1.6955)  time: 0.0262  data: 0.0000\n",
      "Epoch: [31]  [250/349]  eta: 0:00:02  loss_g: -1.5526 (-1.5899)  loss_d: -0.0488 (-0.1029)  loss: -1.6844 (-1.6928)  time: 0.0274  data: 0.0000\n",
      "Epoch: [31]  [300/349]  eta: 0:00:01  loss_g: -1.0417 (-1.5197)  loss_d: -0.2335 (-0.1061)  loss: -1.1568 (-1.6258)  time: 0.0263  data: 0.0000\n",
      "Epoch: [31]  [349/349]  eta: 0:00:00  loss_g: -1.4490 (-1.5007)  loss_d: 0.0776 (-0.1093)  loss: -1.3370 (-1.6101)  time: 0.0294  data: 0.0000\n",
      "Epoch: [31] Total time: 0:00:09\n",
      "Epoch: [32]  [  0/349]  eta: 0:00:10  loss_g: -1.8342 (-1.8342)  loss_d: -0.1543 (-0.1543)  loss: -1.9884 (-1.9884)  time: 0.0293  data: 0.0000\n",
      "Epoch: [32]  [ 50/349]  eta: 0:00:08  loss_g: -1.5686 (-1.5944)  loss_d: -0.1998 (-0.0389)  loss: -1.7287 (-1.6333)  time: 0.0299  data: 0.0000\n",
      "Epoch: [32]  [100/349]  eta: 0:00:07  loss_g: -1.1816 (-1.3558)  loss_d: 0.0404 (-0.0236)  loss: -1.1412 (-1.3794)  time: 0.0258  data: 0.0000\n",
      "Epoch: [32]  [150/349]  eta: 0:00:05  loss_g: -1.1540 (-1.2614)  loss_d: 0.0366 (-0.0114)  loss: -1.0275 (-1.2728)  time: 0.0267  data: 0.0000\n",
      "Epoch: [32]  [200/349]  eta: 0:00:04  loss_g: -1.4945 (-1.3148)  loss_d: -0.0813 (-0.0432)  loss: -1.6525 (-1.3580)  time: 0.0262  data: 0.0000\n",
      "Epoch: [32]  [250/349]  eta: 0:00:02  loss_g: -1.0656 (-1.3018)  loss_d: -0.2103 (-0.0564)  loss: -1.3962 (-1.3582)  time: 0.0266  data: 0.0000\n",
      "Epoch: [32]  [300/349]  eta: 0:00:01  loss_g: -0.9396 (-1.2722)  loss_d: -0.1160 (-0.0449)  loss: -1.1877 (-1.3171)  time: 0.0260  data: 0.0000\n",
      "Epoch: [32]  [349/349]  eta: 0:00:00  loss_g: -1.1457 (-1.2686)  loss_d: -0.1536 (-0.0431)  loss: -1.4136 (-1.3116)  time: 0.0259  data: 0.0000\n",
      "Epoch: [32] Total time: 0:00:09\n",
      "Epoch: [33]  [  0/349]  eta: 0:00:10  loss_g: -0.7669 (-0.7669)  loss_d: -0.6037 (-0.6037)  loss: -1.3706 (-1.3706)  time: 0.0299  data: 0.0000\n",
      "Epoch: [33]  [ 50/349]  eta: 0:00:07  loss_g: -1.3049 (-1.3823)  loss_d: -0.0870 (-0.0274)  loss: -1.3919 (-1.4098)  time: 0.0257  data: 0.0000\n",
      "Epoch: [33]  [100/349]  eta: 0:00:06  loss_g: -1.2662 (-1.3465)  loss_d: -0.1354 (0.0082)  loss: -1.2627 (-1.3383)  time: 0.0262  data: 0.0000\n",
      "Epoch: [33]  [150/349]  eta: 0:00:05  loss_g: -1.3058 (-1.2811)  loss_d: -0.1100 (-0.0156)  loss: -1.5290 (-1.2967)  time: 0.0260  data: 0.0000\n",
      "Epoch: [33]  [200/349]  eta: 0:00:03  loss_g: -1.5225 (-1.3918)  loss_d: 0.0587 (-0.0127)  loss: -1.5541 (-1.4045)  time: 0.0258  data: 0.0000\n",
      "Epoch: [33]  [250/349]  eta: 0:00:02  loss_g: -1.3388 (-1.3892)  loss_d: -0.0874 (-0.0252)  loss: -1.2402 (-1.4144)  time: 0.0261  data: 0.0000\n",
      "Epoch: [33]  [300/349]  eta: 0:00:01  loss_g: -1.4810 (-1.3722)  loss_d: -0.0774 (-0.0223)  loss: -1.4492 (-1.3945)  time: 0.0258  data: 0.0000\n",
      "Epoch: [33]  [349/349]  eta: 0:00:00  loss_g: -1.8536 (-1.4315)  loss_d: -0.0927 (-0.0254)  loss: -1.8972 (-1.4569)  time: 0.0270  data: 0.0000\n",
      "Epoch: [33] Total time: 0:00:09\n",
      "Epoch: [34]  [  0/349]  eta: 0:00:09  loss_g: -1.4178 (-1.4178)  loss_d: -0.4915 (-0.4915)  loss: -1.9093 (-1.9093)  time: 0.0267  data: 0.0000\n",
      "Epoch: [34]  [ 50/349]  eta: 0:00:08  loss_g: -1.1858 (-1.1949)  loss_d: -0.0209 (-0.0874)  loss: -1.1794 (-1.2823)  time: 0.0262  data: 0.0000\n",
      "Epoch: [34]  [100/349]  eta: 0:00:06  loss_g: -1.4102 (-1.2920)  loss_d: -0.1832 (-0.0967)  loss: -1.6516 (-1.3886)  time: 0.0269  data: 0.0000\n",
      "Epoch: [34]  [150/349]  eta: 0:00:05  loss_g: -1.5285 (-1.3269)  loss_d: -0.2689 (-0.0809)  loss: -1.5104 (-1.4078)  time: 0.0271  data: 0.0000\n",
      "Epoch: [34]  [200/349]  eta: 0:00:04  loss_g: -1.1483 (-1.3070)  loss_d: -0.0859 (-0.0915)  loss: -1.1355 (-1.3985)  time: 0.0283  data: 0.0000\n",
      "Epoch: [34]  [250/349]  eta: 0:00:02  loss_g: -1.3346 (-1.2862)  loss_d: -0.0619 (-0.0871)  loss: -1.4417 (-1.3732)  time: 0.0284  data: 0.0000\n",
      "Epoch: [34]  [300/349]  eta: 0:00:01  loss_g: -1.3358 (-1.2938)  loss_d: -0.1225 (-0.0775)  loss: -1.4342 (-1.3713)  time: 0.0292  data: 0.0000\n",
      "Epoch: [34]  [349/349]  eta: 0:00:00  loss_g: -1.4459 (-1.2936)  loss_d: 0.0762 (-0.0672)  loss: -1.4799 (-1.3608)  time: 0.0284  data: 0.0000\n",
      "Epoch: [34] Total time: 0:00:09\n",
      "Epoch: [35]  [  0/349]  eta: 0:00:09  loss_g: -1.3904 (-1.3904)  loss_d: 0.1374 (0.1374)  loss: -1.2530 (-1.2530)  time: 0.0268  data: 0.0000\n",
      "Epoch: [35]  [ 50/349]  eta: 0:00:08  loss_g: -1.5729 (-1.5068)  loss_d: 0.0670 (-0.0802)  loss: -1.5426 (-1.5869)  time: 0.0277  data: 0.0000\n",
      "Epoch: [35]  [100/349]  eta: 0:00:06  loss_g: -1.2562 (-1.4785)  loss_d: -0.0172 (0.0051)  loss: -1.3730 (-1.4734)  time: 0.0265  data: 0.0000\n",
      "Epoch: [35]  [150/349]  eta: 0:00:05  loss_g: -1.3584 (-1.3941)  loss_d: 0.0605 (-0.0088)  loss: -1.3573 (-1.4029)  time: 0.0183  data: 0.0000\n",
      "Epoch: [35]  [200/349]  eta: 0:00:03  loss_g: -1.1027 (-1.3997)  loss_d: -0.1983 (-0.0389)  loss: -1.2992 (-1.4386)  time: 0.0173  data: 0.0000\n",
      "Epoch: [35]  [250/349]  eta: 0:00:02  loss_g: -1.3028 (-1.3343)  loss_d: -0.1577 (-0.0532)  loss: -1.6643 (-1.3875)  time: 0.0175  data: 0.0000\n",
      "Epoch: [35]  [300/349]  eta: 0:00:01  loss_g: -1.6036 (-1.3763)  loss_d: -0.2303 (-0.0481)  loss: -1.6328 (-1.4244)  time: 0.0170  data: 0.0000\n",
      "Epoch: [35]  [349/349]  eta: 0:00:00  loss_g: -1.4370 (-1.3755)  loss_d: -0.0438 (-0.0616)  loss: -1.4326 (-1.4371)  time: 0.0172  data: 0.0000\n",
      "Epoch: [35] Total time: 0:00:07\n",
      "Epoch: [36]  [  0/349]  eta: 0:00:05  loss_g: -1.2366 (-1.2366)  loss_d: 0.6161 (0.6161)  loss: -0.6205 (-0.6205)  time: 0.0170  data: 0.0000\n",
      "Epoch: [36]  [ 50/349]  eta: 0:00:05  loss_g: -1.3173 (-1.3878)  loss_d: -0.0639 (-0.0891)  loss: -1.4129 (-1.4769)  time: 0.0178  data: 0.0000\n",
      "Epoch: [36]  [100/349]  eta: 0:00:04  loss_g: -1.1731 (-1.3236)  loss_d: -0.1573 (-0.0855)  loss: -1.5139 (-1.4091)  time: 0.0177  data: 0.0000\n",
      "Epoch: [36]  [150/349]  eta: 0:00:03  loss_g: -1.1756 (-1.3332)  loss_d: -0.2609 (-0.0659)  loss: -1.3745 (-1.3991)  time: 0.0179  data: 0.0000\n",
      "Epoch: [36]  [200/349]  eta: 0:00:02  loss_g: -1.1494 (-1.2908)  loss_d: -0.1102 (-0.0692)  loss: -1.2474 (-1.3600)  time: 0.0177  data: 0.0000\n",
      "Epoch: [36]  [250/349]  eta: 0:00:01  loss_g: -1.1295 (-1.2640)  loss_d: -0.0535 (-0.0520)  loss: -1.1549 (-1.3160)  time: 0.0173  data: 0.0000\n",
      "Epoch: [36]  [300/349]  eta: 0:00:00  loss_g: -1.1191 (-1.2284)  loss_d: 0.0393 (-0.0268)  loss: -1.0875 (-1.2552)  time: 0.0185  data: 0.0000\n",
      "Epoch: [36]  [349/349]  eta: 0:00:00  loss_g: -1.2278 (-1.2416)  loss_d: 0.0119 (-0.0163)  loss: -1.2438 (-1.2579)  time: 0.0176  data: 0.0000\n",
      "Epoch: [36] Total time: 0:00:06\n",
      "Epoch: [37]  [  0/349]  eta: 0:00:06  loss_g: -1.6601 (-1.6601)  loss_d: -0.1946 (-0.1946)  loss: -1.8547 (-1.8547)  time: 0.0174  data: 0.0000\n",
      "Epoch: [37]  [ 50/349]  eta: 0:00:05  loss_g: -1.3433 (-1.1392)  loss_d: -0.1925 (-0.1248)  loss: -1.4521 (-1.2641)  time: 0.0180  data: 0.0000\n",
      "Epoch: [37]  [100/349]  eta: 0:00:04  loss_g: -1.0628 (-1.1546)  loss_d: -0.0599 (-0.0847)  loss: -0.8824 (-1.2393)  time: 0.0175  data: 0.0000\n",
      "Epoch: [37]  [150/349]  eta: 0:00:03  loss_g: -1.1531 (-1.0150)  loss_d: -0.0183 (-0.0694)  loss: -1.0424 (-1.0845)  time: 0.0204  data: 0.0000\n",
      "Epoch: [37]  [200/349]  eta: 0:00:02  loss_g: -1.5203 (-1.1423)  loss_d: -0.3136 (-0.0839)  loss: -1.7640 (-1.2262)  time: 0.0180  data: 0.0000\n",
      "Epoch: [37]  [250/349]  eta: 0:00:01  loss_g: -1.1153 (-1.1610)  loss_d: -0.0470 (-0.0638)  loss: -1.2807 (-1.2247)  time: 0.0187  data: 0.0000\n",
      "Epoch: [37]  [300/349]  eta: 0:00:00  loss_g: -1.3845 (-1.1864)  loss_d: -0.2002 (-0.0571)  loss: -1.4353 (-1.2435)  time: 0.0187  data: 0.0000\n",
      "Epoch: [37]  [349/349]  eta: 0:00:00  loss_g: -1.1338 (-1.1925)  loss_d: -0.1004 (-0.0643)  loss: -1.3046 (-1.2568)  time: 0.0182  data: 0.0000\n",
      "Epoch: [37] Total time: 0:00:06\n",
      "Epoch: [38]  [  0/349]  eta: 0:00:06  loss_g: -1.1712 (-1.1712)  loss_d: 0.1883 (0.1883)  loss: -0.9829 (-0.9829)  time: 0.0171  data: 0.0000\n",
      "Epoch: [38]  [ 50/349]  eta: 0:00:05  loss_g: -0.8026 (-0.8127)  loss_d: -0.3387 (-0.1362)  loss: -1.0314 (-0.9490)  time: 0.0177  data: 0.0000\n",
      "Epoch: [38]  [100/349]  eta: 0:00:04  loss_g: -1.3649 (-0.9983)  loss_d: -0.0800 (-0.0473)  loss: -1.5515 (-1.0455)  time: 0.0179  data: 0.0000\n",
      "Epoch: [38]  [150/349]  eta: 0:00:03  loss_g: -1.1718 (-1.1065)  loss_d: -0.0793 (-0.0343)  loss: -1.1946 (-1.1408)  time: 0.0177  data: 0.0000\n",
      "Epoch: [38]  [200/349]  eta: 0:00:02  loss_g: -1.2870 (-1.1447)  loss_d: 0.0384 (-0.0378)  loss: -1.2004 (-1.1825)  time: 0.0179  data: 0.0000\n",
      "Epoch: [38]  [250/349]  eta: 0:00:01  loss_g: -1.2446 (-1.1717)  loss_d: -0.2093 (-0.0415)  loss: -1.3370 (-1.2132)  time: 0.0181  data: 0.0000\n",
      "Epoch: [38]  [300/349]  eta: 0:00:00  loss_g: -1.5453 (-1.2391)  loss_d: -0.1706 (-0.0384)  loss: -1.6318 (-1.2775)  time: 0.0178  data: 0.0000\n",
      "Epoch: [38]  [349/349]  eta: 0:00:00  loss_g: -1.2257 (-1.2212)  loss_d: -0.0436 (-0.0377)  loss: -1.1810 (-1.2588)  time: 0.0179  data: 0.0000\n",
      "Epoch: [38] Total time: 0:00:06\n",
      "Epoch: [39]  [  0/349]  eta: 0:00:07  loss_g: -1.3607 (-1.3607)  loss_d: -0.1539 (-0.1539)  loss: -1.5146 (-1.5146)  time: 0.0201  data: 0.0000\n",
      "Epoch: [39]  [ 50/349]  eta: 0:00:05  loss_g: -1.4376 (-1.3901)  loss_d: -0.0415 (0.0198)  loss: -1.3689 (-1.3703)  time: 0.0180  data: 0.0000\n",
      "Epoch: [39]  [100/349]  eta: 0:00:04  loss_g: -1.3418 (-1.3319)  loss_d: -0.1267 (-0.0585)  loss: -1.4339 (-1.3904)  time: 0.0186  data: 0.0000\n",
      "Epoch: [39]  [150/349]  eta: 0:00:03  loss_g: -1.3973 (-1.3197)  loss_d: -0.0834 (-0.0518)  loss: -1.6101 (-1.3716)  time: 0.0182  data: 0.0000\n",
      "Epoch: [39]  [200/349]  eta: 0:00:02  loss_g: -1.2999 (-1.3220)  loss_d: -0.0901 (-0.0407)  loss: -1.4195 (-1.3627)  time: 0.0180  data: 0.0000\n",
      "Epoch: [39]  [250/349]  eta: 0:00:01  loss_g: -1.1915 (-1.2848)  loss_d: -0.2065 (-0.0402)  loss: -1.3174 (-1.3249)  time: 0.0173  data: 0.0000\n",
      "Epoch: [39]  [300/349]  eta: 0:00:00  loss_g: -1.4285 (-1.2972)  loss_d: 0.2023 (-0.0205)  loss: -1.2727 (-1.3177)  time: 0.0175  data: 0.0000\n",
      "Epoch: [39]  [349/349]  eta: 0:00:00  loss_g: -1.0757 (-1.2916)  loss_d: -0.0156 (-0.0135)  loss: -1.3040 (-1.3051)  time: 0.0176  data: 0.0000\n",
      "Epoch: [39] Total time: 0:00:06\n",
      "Epoch: [40]  [  0/349]  eta: 0:00:05  loss_g: -0.8262 (-0.8262)  loss_d: -0.9219 (-0.9219)  loss: -1.7481 (-1.7481)  time: 0.0171  data: 0.0000\n",
      "Epoch: [40]  [ 50/349]  eta: 0:00:05  loss_g: -1.4849 (-1.3437)  loss_d: -0.1832 (-0.1344)  loss: -1.5988 (-1.4781)  time: 0.0176  data: 0.0000\n",
      "Epoch: [40]  [100/349]  eta: 0:00:04  loss_g: -1.2956 (-1.3797)  loss_d: 0.2157 (-0.0450)  loss: -1.2127 (-1.4247)  time: 0.0173  data: 0.0000\n",
      "Epoch: [40]  [150/349]  eta: 0:00:03  loss_g: -0.9096 (-1.2819)  loss_d: 0.0044 (-0.0464)  loss: -0.8025 (-1.3283)  time: 0.0180  data: 0.0000\n",
      "Epoch: [40]  [200/349]  eta: 0:00:02  loss_g: -1.5858 (-1.3079)  loss_d: -0.0443 (-0.0308)  loss: -1.5889 (-1.3387)  time: 0.0180  data: 0.0000\n",
      "Epoch: [40]  [250/349]  eta: 0:00:01  loss_g: -1.4709 (-1.3343)  loss_d: 0.1117 (-0.0133)  loss: -1.3366 (-1.3476)  time: 0.0183  data: 0.0000\n",
      "Epoch: [40]  [300/349]  eta: 0:00:00  loss_g: -0.9187 (-1.2620)  loss_d: -0.0437 (-0.0143)  loss: -0.8837 (-1.2763)  time: 0.0184  data: 0.0000\n",
      "Epoch: [40]  [349/349]  eta: 0:00:00  loss_g: -1.5914 (-1.2792)  loss_d: 0.0047 (-0.0196)  loss: -1.5793 (-1.2987)  time: 0.0187  data: 0.0000\n",
      "Epoch: [40] Total time: 0:00:06\n",
      "Epoch: [41]  [  0/349]  eta: 0:00:06  loss_g: -1.3540 (-1.3540)  loss_d: -0.2127 (-0.2127)  loss: -1.5667 (-1.5667)  time: 0.0178  data: 0.0000\n",
      "Epoch: [41]  [ 50/349]  eta: 0:00:05  loss_g: -1.1247 (-1.2829)  loss_d: 0.1067 (0.0096)  loss: -1.0151 (-1.2733)  time: 0.0189  data: 0.0000\n",
      "Epoch: [41]  [100/349]  eta: 0:00:04  loss_g: -1.1890 (-1.1765)  loss_d: -0.0342 (-0.0029)  loss: -1.1520 (-1.1794)  time: 0.0183  data: 0.0000\n",
      "Epoch: [41]  [150/349]  eta: 0:00:03  loss_g: -1.7866 (-1.3131)  loss_d: 0.0112 (0.0017)  loss: -1.9229 (-1.3114)  time: 0.0178  data: 0.0000\n",
      "Epoch: [41]  [200/349]  eta: 0:00:02  loss_g: -1.1221 (-1.3376)  loss_d: -0.1905 (-0.0294)  loss: -1.3906 (-1.3671)  time: 0.0178  data: 0.0000\n",
      "Epoch: [41]  [250/349]  eta: 0:00:01  loss_g: -1.6505 (-1.3388)  loss_d: -0.1285 (-0.0291)  loss: -1.5692 (-1.3678)  time: 0.0179  data: 0.0000\n",
      "Epoch: [41]  [300/349]  eta: 0:00:00  loss_g: -1.5152 (-1.3785)  loss_d: -0.1297 (-0.0369)  loss: -1.6427 (-1.4154)  time: 0.0179  data: 0.0000\n",
      "Epoch: [41]  [349/349]  eta: 0:00:00  loss_g: -1.0723 (-1.3647)  loss_d: 0.0004 (-0.0264)  loss: -1.2053 (-1.3912)  time: 0.0180  data: 0.0000\n",
      "Epoch: [41] Total time: 0:00:06\n",
      "Epoch: [42]  [  0/349]  eta: 0:00:05  loss_g: -0.4829 (-0.4829)  loss_d: 0.2111 (0.2111)  loss: -0.2718 (-0.2718)  time: 0.0166  data: 0.0000\n",
      "Epoch: [42]  [ 50/349]  eta: 0:00:05  loss_g: -1.3159 (-1.0852)  loss_d: 0.0797 (0.0309)  loss: -1.2882 (-1.0543)  time: 0.0178  data: 0.0000\n",
      "Epoch: [42]  [100/349]  eta: 0:00:04  loss_g: -1.2186 (-1.2862)  loss_d: -0.0934 (-0.0177)  loss: -1.4372 (-1.3038)  time: 0.0176  data: 0.0000\n",
      "Epoch: [42]  [150/349]  eta: 0:00:03  loss_g: -1.0335 (-1.2245)  loss_d: -0.1589 (-0.0247)  loss: -1.1091 (-1.2492)  time: 0.0183  data: 0.0000\n",
      "Epoch: [42]  [200/349]  eta: 0:00:02  loss_g: -1.4594 (-1.2175)  loss_d: 0.0106 (-0.0294)  loss: -1.4688 (-1.2469)  time: 0.0180  data: 0.0000\n",
      "Epoch: [42]  [250/349]  eta: 0:00:01  loss_g: -1.5979 (-1.3137)  loss_d: -0.0571 (-0.0430)  loss: -1.7730 (-1.3567)  time: 0.0179  data: 0.0000\n",
      "Epoch: [42]  [300/349]  eta: 0:00:00  loss_g: -1.1826 (-1.3026)  loss_d: -0.0573 (-0.0399)  loss: -1.2998 (-1.3425)  time: 0.0172  data: 0.0000\n",
      "Epoch: [42]  [349/349]  eta: 0:00:00  loss_g: -1.4124 (-1.2852)  loss_d: -0.1360 (-0.0498)  loss: -1.6388 (-1.3350)  time: 0.0175  data: 0.0000\n",
      "Epoch: [42] Total time: 0:00:06\n",
      "Epoch: [43]  [  0/349]  eta: 0:00:06  loss_g: -1.4424 (-1.4424)  loss_d: -0.3032 (-0.3032)  loss: -1.7456 (-1.7456)  time: 0.0172  data: 0.0000\n",
      "Epoch: [43]  [ 50/349]  eta: 0:00:05  loss_g: -1.6703 (-1.8233)  loss_d: -0.0357 (-0.0123)  loss: -1.7343 (-1.8355)  time: 0.0174  data: 0.0000\n",
      "Epoch: [43]  [100/349]  eta: 0:00:04  loss_g: -1.3841 (-1.5930)  loss_d: -0.1378 (-0.0752)  loss: -1.4496 (-1.6682)  time: 0.0174  data: 0.0000\n",
      "Epoch: [43]  [150/349]  eta: 0:00:03  loss_g: -1.3830 (-1.5055)  loss_d: -0.1013 (-0.0885)  loss: -1.5091 (-1.5940)  time: 0.0176  data: 0.0000\n",
      "Epoch: [43]  [200/349]  eta: 0:00:02  loss_g: -1.6596 (-1.5080)  loss_d: -0.1823 (-0.0905)  loss: -1.5830 (-1.5985)  time: 0.0174  data: 0.0000\n",
      "Epoch: [43]  [250/349]  eta: 0:00:01  loss_g: -0.9301 (-1.4138)  loss_d: -0.3186 (-0.1065)  loss: -1.1740 (-1.5203)  time: 0.0180  data: 0.0000\n",
      "Epoch: [43]  [300/349]  eta: 0:00:00  loss_g: -1.7762 (-1.4398)  loss_d: -0.1262 (-0.0770)  loss: -1.8704 (-1.5168)  time: 0.0196  data: 0.0000\n",
      "Epoch: [43]  [349/349]  eta: 0:00:00  loss_g: -1.5666 (-1.4738)  loss_d: -0.2353 (-0.0766)  loss: -1.7153 (-1.5504)  time: 0.0179  data: 0.0000\n",
      "Epoch: [43] Total time: 0:00:06\n",
      "Epoch: [44]  [  0/349]  eta: 0:00:06  loss_g: -1.5804 (-1.5804)  loss_d: -0.5478 (-0.5478)  loss: -2.1282 (-2.1282)  time: 0.0173  data: 0.0000\n",
      "Epoch: [44]  [ 50/349]  eta: 0:00:05  loss_g: -0.8590 (-1.0390)  loss_d: -0.0720 (-0.1364)  loss: -0.9651 (-1.1754)  time: 0.0184  data: 0.0000\n",
      "Epoch: [44]  [100/349]  eta: 0:00:04  loss_g: -1.5455 (-1.2360)  loss_d: -0.0639 (-0.0545)  loss: -1.6521 (-1.2905)  time: 0.0184  data: 0.0000\n",
      "Epoch: [44]  [150/349]  eta: 0:00:03  loss_g: -1.2438 (-1.2508)  loss_d: -0.0207 (-0.0473)  loss: -1.0256 (-1.2980)  time: 0.0180  data: 0.0000\n",
      "Epoch: [44]  [200/349]  eta: 0:00:02  loss_g: -1.3199 (-1.2597)  loss_d: 0.0407 (-0.0141)  loss: -1.2667 (-1.2738)  time: 0.0181  data: 0.0000\n",
      "Epoch: [44]  [250/349]  eta: 0:00:01  loss_g: -1.3024 (-1.2694)  loss_d: 0.0145 (-0.0219)  loss: -1.2879 (-1.2913)  time: 0.0180  data: 0.0000\n",
      "Epoch: [44]  [300/349]  eta: 0:00:00  loss_g: -1.2725 (-1.2823)  loss_d: -0.1113 (-0.0117)  loss: -1.2262 (-1.2940)  time: 0.0191  data: 0.0000\n",
      "Epoch: [44]  [349/349]  eta: 0:00:00  loss_g: -1.3321 (-1.2641)  loss_d: -0.0561 (-0.0165)  loss: -1.3148 (-1.2806)  time: 0.0175  data: 0.0000\n",
      "Epoch: [44] Total time: 0:00:06\n",
      "Epoch: [45]  [  0/349]  eta: 0:00:05  loss_g: -1.5224 (-1.5224)  loss_d: 0.2453 (0.2453)  loss: -1.2770 (-1.2770)  time: 0.0171  data: 0.0000\n",
      "Epoch: [45]  [ 50/349]  eta: 0:00:05  loss_g: -1.4663 (-1.6966)  loss_d: -0.0385 (-0.0405)  loss: -1.4883 (-1.7371)  time: 0.0181  data: 0.0000\n",
      "Epoch: [45]  [100/349]  eta: 0:00:04  loss_g: -0.9874 (-1.4931)  loss_d: -0.1218 (-0.0094)  loss: -1.1024 (-1.5025)  time: 0.0179  data: 0.0000\n",
      "Epoch: [45]  [150/349]  eta: 0:00:03  loss_g: -1.3698 (-1.4083)  loss_d: -0.0607 (-0.0092)  loss: -1.4250 (-1.4175)  time: 0.0186  data: 0.0000\n",
      "Epoch: [45]  [200/349]  eta: 0:00:02  loss_g: -1.4836 (-1.4343)  loss_d: -0.1739 (-0.0294)  loss: -1.7091 (-1.4637)  time: 0.0183  data: 0.0000\n",
      "Epoch: [45]  [250/349]  eta: 0:00:01  loss_g: -0.8606 (-1.3274)  loss_d: -0.1409 (-0.0397)  loss: -0.7027 (-1.3671)  time: 0.0180  data: 0.0000\n",
      "Epoch: [45]  [300/349]  eta: 0:00:00  loss_g: -2.0388 (-1.3868)  loss_d: -0.1009 (-0.0450)  loss: -2.1433 (-1.4318)  time: 0.0183  data: 0.0000\n",
      "Epoch: [45]  [349/349]  eta: 0:00:00  loss_g: -1.0384 (-1.3896)  loss_d: -0.0495 (-0.0391)  loss: -1.1554 (-1.4287)  time: 0.0185  data: 0.0000\n",
      "Epoch: [45] Total time: 0:00:06\n",
      "Epoch: [46]  [  0/349]  eta: 0:00:06  loss_g: -1.0915 (-1.0915)  loss_d: -0.2692 (-0.2692)  loss: -1.3607 (-1.3607)  time: 0.0181  data: 0.0000\n",
      "Epoch: [46]  [ 50/349]  eta: 0:00:05  loss_g: -1.3518 (-1.2056)  loss_d: 0.0561 (0.0036)  loss: -1.2941 (-1.2021)  time: 0.0180  data: 0.0000\n",
      "Epoch: [46]  [100/349]  eta: 0:00:04  loss_g: -1.6223 (-1.3689)  loss_d: 0.0820 (-0.0442)  loss: -1.5768 (-1.4130)  time: 0.0186  data: 0.0000\n",
      "Epoch: [46]  [150/349]  eta: 0:00:03  loss_g: -1.4824 (-1.4753)  loss_d: -0.1565 (-0.0580)  loss: -1.7211 (-1.5333)  time: 0.0180  data: 0.0000\n",
      "Epoch: [46]  [200/349]  eta: 0:00:02  loss_g: -1.0829 (-1.3741)  loss_d: -0.0056 (-0.0424)  loss: -1.1101 (-1.4164)  time: 0.0179  data: 0.0000\n",
      "Epoch: [46]  [250/349]  eta: 0:00:01  loss_g: -1.5764 (-1.4223)  loss_d: -0.1406 (-0.0546)  loss: -1.7126 (-1.4769)  time: 0.0173  data: 0.0000\n",
      "Epoch: [46]  [300/349]  eta: 0:00:00  loss_g: -0.9513 (-1.3822)  loss_d: -0.0026 (-0.0436)  loss: -0.9704 (-1.4258)  time: 0.0174  data: 0.0000\n",
      "Epoch: [46]  [349/349]  eta: 0:00:00  loss_g: -1.2313 (-1.3481)  loss_d: -0.0115 (-0.0218)  loss: -1.3289 (-1.3699)  time: 0.0194  data: 0.0000\n",
      "Epoch: [46] Total time: 0:00:06\n",
      "Epoch: [47]  [  0/349]  eta: 0:00:05  loss_g: -1.3979 (-1.3979)  loss_d: 0.0253 (0.0253)  loss: -1.3726 (-1.3726)  time: 0.0167  data: 0.0000\n",
      "Epoch: [47]  [ 50/349]  eta: 0:00:05  loss_g: -1.9179 (-1.7857)  loss_d: 0.0222 (0.0646)  loss: -1.8878 (-1.7211)  time: 0.0179  data: 0.0000\n",
      "Epoch: [47]  [100/349]  eta: 0:00:04  loss_g: -1.4408 (-1.6669)  loss_d: -0.0857 (-0.0133)  loss: -1.5275 (-1.6802)  time: 0.0183  data: 0.0000\n",
      "Epoch: [47]  [150/349]  eta: 0:00:03  loss_g: -1.5813 (-1.5826)  loss_d: -0.1385 (-0.0416)  loss: -1.6150 (-1.6242)  time: 0.0187  data: 0.0000\n",
      "Epoch: [47]  [200/349]  eta: 0:00:02  loss_g: -1.6835 (-1.5993)  loss_d: -0.0640 (-0.0497)  loss: -1.7958 (-1.6490)  time: 0.0186  data: 0.0000\n",
      "Epoch: [47]  [250/349]  eta: 0:00:01  loss_g: -1.0628 (-1.5429)  loss_d: -0.0554 (-0.0461)  loss: -1.2243 (-1.5889)  time: 0.0186  data: 0.0000\n",
      "Epoch: [47]  [300/349]  eta: 0:00:00  loss_g: -1.8978 (-1.5524)  loss_d: -0.1377 (-0.0437)  loss: -2.0235 (-1.5961)  time: 0.0179  data: 0.0000\n",
      "Epoch: [47]  [349/349]  eta: 0:00:00  loss_g: -1.6503 (-1.5722)  loss_d: 0.0089 (-0.0432)  loss: -1.6534 (-1.6154)  time: 0.0194  data: 0.0000\n",
      "Epoch: [47] Total time: 0:00:06\n",
      "Epoch: [48]  [  0/349]  eta: 0:00:05  loss_g: -1.8667 (-1.8667)  loss_d: 0.0356 (0.0356)  loss: -1.8311 (-1.8311)  time: 0.0164  data: 0.0000\n",
      "Epoch: [48]  [ 50/349]  eta: 0:00:05  loss_g: -1.1954 (-1.2567)  loss_d: -0.0755 (-0.0624)  loss: -1.2506 (-1.3191)  time: 0.0175  data: 0.0000\n",
      "Epoch: [48]  [100/349]  eta: 0:00:04  loss_g: -1.4620 (-1.3415)  loss_d: -0.2138 (-0.0675)  loss: -1.7368 (-1.4090)  time: 0.0178  data: 0.0000\n",
      "Epoch: [48]  [150/349]  eta: 0:00:03  loss_g: -1.5812 (-1.4453)  loss_d: -0.1407 (-0.0549)  loss: -1.5218 (-1.5002)  time: 0.0178  data: 0.0000\n",
      "Epoch: [48]  [200/349]  eta: 0:00:02  loss_g: -1.1295 (-1.3908)  loss_d: 0.0036 (-0.0427)  loss: -1.0955 (-1.4335)  time: 0.0178  data: 0.0000\n",
      "Epoch: [48]  [250/349]  eta: 0:00:01  loss_g: -1.6417 (-1.4034)  loss_d: -0.1547 (-0.0451)  loss: -1.8501 (-1.4486)  time: 0.0176  data: 0.0000\n",
      "Epoch: [48]  [300/349]  eta: 0:00:00  loss_g: -1.5126 (-1.4323)  loss_d: -0.0197 (-0.0321)  loss: -1.4916 (-1.4643)  time: 0.0174  data: 0.0000\n",
      "Epoch: [48]  [349/349]  eta: 0:00:00  loss_g: -1.5925 (-1.4443)  loss_d: -0.0390 (-0.0316)  loss: -1.6016 (-1.4759)  time: 0.0195  data: 0.0000\n",
      "Epoch: [48] Total time: 0:00:06\n",
      "Epoch: [49]  [  0/349]  eta: 0:00:07  loss_g: -1.4468 (-1.4468)  loss_d: -0.4984 (-0.4984)  loss: -1.9451 (-1.9451)  time: 0.0208  data: 0.0000\n",
      "Epoch: [49]  [ 50/349]  eta: 0:00:05  loss_g: -1.7618 (-1.7696)  loss_d: -0.0190 (-0.0846)  loss: -1.7051 (-1.8542)  time: 0.0185  data: 0.0000\n",
      "Epoch: [49]  [100/349]  eta: 0:00:04  loss_g: -1.7155 (-1.6798)  loss_d: -0.0761 (-0.0844)  loss: -1.6763 (-1.7642)  time: 0.0182  data: 0.0000\n",
      "Epoch: [49]  [150/349]  eta: 0:00:03  loss_g: -1.6607 (-1.6835)  loss_d: -0.0876 (-0.0879)  loss: -1.8004 (-1.7713)  time: 0.0183  data: 0.0000\n",
      "Epoch: [49]  [200/349]  eta: 0:00:02  loss_g: -1.6129 (-1.6614)  loss_d: 0.0367 (-0.0858)  loss: -1.6667 (-1.7472)  time: 0.0176  data: 0.0000\n",
      "Epoch: [49]  [250/349]  eta: 0:00:01  loss_g: -1.3764 (-1.6241)  loss_d: -0.1282 (-0.0899)  loss: -1.5236 (-1.7141)  time: 0.0174  data: 0.0000\n",
      "Epoch: [49]  [300/349]  eta: 0:00:00  loss_g: -1.4263 (-1.5872)  loss_d: -0.0086 (-0.0780)  loss: -1.3510 (-1.6652)  time: 0.0175  data: 0.0000\n",
      "Epoch: [49]  [349/349]  eta: 0:00:00  loss_g: -1.3356 (-1.5794)  loss_d: -0.0219 (-0.0673)  loss: -1.5263 (-1.6467)  time: 0.0176  data: 0.0000\n",
      "Epoch: [49] Total time: 0:00:06\n"
     ]
    }
   ],
   "source": [
    "synth = TwinSynthesizer(batch_size=200,device='cpu')\n",
    "synth.fit(data=df,epochs=50,discrete_columns=[])\n",
    "sample = synth.sample(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Synthetic Data -  0.6496411779262559\n"
     ]
    }
   ],
   "source": [
    "synth_Y = sample['cardio']\n",
    "synth_X = sample.drop('cardio', axis=1)\n",
    "model.fit(synth_X, synth_Y)\n",
    "print(\"F1 Score on Synthetic Data - \", get_f1_score(test_X, test_Y, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on Synthetic and Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Real + Synthetic Data -  0.7205735909181438\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([train_X, synth_X], ignore_index=True)\n",
    "Y = pd.concat([train_Y, synth_Y], ignore_index=True)\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"F1 Score on Real + Synthetic Data - \", get_f1_score(test_X, test_Y, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We see a slight improvement after fitting the model on Real + Synthetic data. With some more hyperparameter tuning or some other models we might be able to improve the above score but this test shows that a combination of real and synthetic data can improve your overall model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1941d862017c0e671b65735c9f23d80bfba43c94220284ce44ec263442c797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
